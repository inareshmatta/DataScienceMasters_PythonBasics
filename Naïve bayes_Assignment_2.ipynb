{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47a6b7be",
   "metadata": {},
   "source": [
    "Q1. A company conducted a survey of its employees and found that 70% of the employees use the  \n",
    "company's health insurance plan, while 40% of the employees who use the plan are smokers.    \n",
    "What is theprobability that an employee is a smoker given that he/she uses the health insurance plan?     \n",
    "Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?   \n",
    "Q3. How does Bernoulli Naive Bayes handle missing values?  \n",
    "Q4. Can Gaussian Naive Bayes be used for multi-class classification?  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f3849c",
   "metadata": {},
   "source": [
    "### Q1. A company conducted a survey of its employees and found that 70% of the employees use the company's health insurance plan, while 40% of the employees who use the plan are smokers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a490f35a",
   "metadata": {},
   "source": [
    "To find the probability that an employee is a smoker given that he/she uses the health insurance plan, we can use Bayes' theorem.\n",
    "\n",
    "Let:\n",
    "- \\( A \\) be the event that an employee uses the health insurance plan.\n",
    "- \\( B \\) be the event that an employee is a smoker.\n",
    "\n",
    "We are given:\n",
    "- \\( P(A) = 0.70 \\) (probability that an employee uses the health insurance plan)\n",
    "- \\( P(B|A) = 0.40 \\) (probability that an employee is a smoker given that they use the health insurance plan)\n",
    "\n",
    "We want to find \\( P(B|A) \\), the probability that an employee is a smoker given that they use the health insurance plan.\n",
    "\n",
    "Using Bayes' theorem:\n",
    "\n",
    "$ P(B|A) = \\frac{P(A|B) \\times P(B)}{P(A)} $\n",
    "\n",
    "We already have \\( P(A|B) = 0.40 \\) (probability that an employee uses the health insurance plan given that they are a smoker), and \\( P(A) = 0.70 \\).\n",
    "\n",
    "We need to find \\( P(B) \\), the probability that an employee is a smoker. We can use the law of total probability to find \\( P(B) \\):\n",
    "\n",
    "$ P(B) = P(B|A) \\times P(A) + P(B|\\neg A) \\times P(\\neg A) $\n",
    "\n",
    "Where:\n",
    "- \\( P(B|A) \\) is the probability that an employee is a smoker given that they use the health insurance plan.\n",
    "- \\( P(\\neg A) \\) is the probability that an employee does not use the health insurance plan (complement of \\( P(A) \\)).\n",
    "- \\( P(B|\\neg A) \\) is the probability that an employee is a smoker given that they do not use the health insurance plan.\n",
    "\n",
    "Given that $( P(\\neg A) = 1 - P(A) = 0.30 )$, and we don't have $( P(B|\\neg A)$ directly, we need more information to solve for $( P(B) )$.\n",
    "\n",
    "If we assume that the probability of an employee being a smoker is independent of whether they use the health insurance plan or not, we can set \\( P(B) = P(B|A) \\). This assumption is often made in the absence of additional information.\n",
    "\n",
    "Using this assumption:\n",
    "\n",
    "\\[ P(B) = P(B|A) = 0.40 \\]\n",
    "\n",
    "Now, we can calculate \\( P(B|A) \\):\n",
    "\n",
    "$ P(B|A) = \\frac{0.40 \\times 0.70}{0.70} = \\boxed{0.40} $\n",
    "\n",
    "So, the probability that an employee is a smoker given that he/she uses the health insurance plan is 0.40, assuming independence between smoking and health insurance plan usage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc28dbd5",
   "metadata": {},
   "source": [
    "### Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19bdb29",
   "metadata": {},
   "source": [
    "The main difference between Bernoulli Naive Bayes and Multinomial Naive Bayes lies in how they model the features and the type of data they are suitable for.\n",
    "\n",
    "1. **Bernoulli Naive Bayes**:\n",
    "   - Bernoulli Naive Bayes assumes that features are binary-valued (e.g., presence or absence of a feature).\n",
    "   - It is commonly used for binary feature vectors, where each feature represents the presence or absence of a particular attribute.\n",
    "   - In text classification, for example, each feature could represent the presence or absence of a word in a document (e.g., \"word is present\" or \"word is absent\").\n",
    "   - Bernoulli Naive Bayes models the likelihood of each feature occurring in each class using a Bernoulli distribution.\n",
    "\n",
    "2. **Multinomial Naive Bayes**:\n",
    "   - Multinomial Naive Bayes assumes that features represent counts or frequencies (e.g., word counts in text).\n",
    "   - It is suitable for datasets where features are represented as integer counts, such as the frequency of words in a document.\n",
    "   - In text classification, each feature typically represents the frequency of a word or term in a document.\n",
    "   - Multinomial Naive Bayes models the likelihood of each feature occurring in each class using a Multinomial distribution.\n",
    "\n",
    "In summary, Bernoulli Naive Bayes is used when features are binary-valued, whereas Multinomial Naive Bayes is used when features represent counts or frequencies. The choice between the two depends on the nature of the data and how the features are represented."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4096d6",
   "metadata": {},
   "source": [
    "### Q3. How does Bernoulli Naive Bayes handle missing values?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c68a93e",
   "metadata": {},
   "source": [
    "Bernoulli Naive Bayes, like many machine learning algorithms, requires complete data without missing values. If your dataset contains missing values, you would typically need to handle them before applying Bernoulli Naive Bayes.\n",
    "\n",
    "Here are some common strategies to handle missing values in a dataset before using Bernoulli Naive Bayes:\n",
    "\n",
    "1. **Imputation**: Replace missing values with a suitable estimate. For binary features, you might impute missing values with the mode (most frequent value) of that feature. However, be cautious with imputation, as it can introduce bias into your dataset.\n",
    "\n",
    "2. **Deletion**: Remove rows or columns with missing values. If the number of missing values is small compared to the size of your dataset, this might be a viable option. However, you risk losing valuable information by deleting observations or features.\n",
    "\n",
    "3. **Model-based imputation**: Use other models to predict missing values based on the non-missing values in the dataset. For example, you could train a separate classifier to predict missing values based on the other features in the dataset.\n",
    "\n",
    "4. **Consider encoding missing values**: In some cases, you might treat missing values as a separate category or encode them with a special value before applying Bernoulli Naive Bayes. However, this approach could potentially introduce noise into your data, so use it judiciously.\n",
    "\n",
    "It's important to carefully consider the implications of each approach and choose the one that best suits your dataset and the underlying problem. Additionally, it's good practice to evaluate the performance of your model after handling missing values to ensure that the chosen approach does not adversely affect the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43069180",
   "metadata": {},
   "source": [
    "### Q4. Can Gaussian Naive Bayes be used for multi-class classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fc7202",
   "metadata": {},
   "source": [
    "Yes, Gaussian Naive Bayes can be used for multi-class classification. In fact, Gaussian Naive Bayes is naturally suited for multi-class classification tasks where the features are continuous and assumed to follow a Gaussian (normal) distribution.\n",
    "\n",
    "In the case of multi-class classification, Gaussian Naive Bayes estimates the parameters (mean and variance) of the Gaussian distribution for each class based on the training data. When making predictions on new data, it calculates the likelihood of the observed features given each class using the Gaussian probability density function. It then combines these likelihoods with the prior probabilities of each class to compute the posterior probability of each class given the observed features.\n",
    "\n",
    "The class with the highest posterior probability is predicted as the output class for the given input features.\n",
    "\n",
    "In summary, Gaussian Naive Bayes can handle multi-class classification by estimating Gaussian distributions for each class and making predictions based on the likelihoods of the observed features. It is a simple and efficient algorithm suitable for classification tasks with continuous features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eececf28",
   "metadata": {},
   "source": [
    "### Q5. Assignment:\n",
    "\n",
    "Data preparation:  \n",
    "Download the \"Spambase Data Set\" from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/\n",
    "datasets/Spambase). This dataset contains email messages, where the goal is to predict whether a message\n",
    "is spam or not based on several input features.     \n",
    "\n",
    "Implementation:\n",
    "Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the\n",
    "scikit-learn library in Python. Use 10-fold cross-validation to evaluate the performance of each classifier on the\n",
    "dataset. You should use the default hyperparameters for each classifier.\n",
    "Results:\n",
    "\n",
    "Report the following performance metrics for each classifier:  \n",
    "Accuracy   \n",
    "Precision   \n",
    "Recall  \n",
    "F1 score    \n",
    "\n",
    "Discussion:   \n",
    "Discuss the results you obtained. Which variant of Naive Bayes performed the best? Why do you think that is\n",
    "the case? Are there any limitations of Naive Bayes that you observed?   \n",
    "\n",
    "\n",
    "Conclusion:    \n",
    "Summarise your findings and provide some suggestions for future work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1ba2211",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\inare\\Anaconda3\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:21: UserWarning: Pandas requires version '2.8.0' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "C:\\Users\\inare\\Anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\masked.py:62: UserWarning: Pandas requires version '1.3.4' or newer of 'bottleneck' (version '1.3.2' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: Bernoulli Naive Bayes\n",
      "Accuracy: 0.8839\n",
      "Precision: 0.8870\n",
      "Recall: 0.8152\n",
      "F1 Score: 0.8481\n",
      "-------------------------------------------\n",
      "Classifier: Multinomial Naive Bayes\n",
      "Accuracy: 0.7863\n",
      "Precision: 0.7393\n",
      "Recall: 0.7215\n",
      "F1 Score: 0.7283\n",
      "-------------------------------------------\n",
      "Classifier: Gaussian Naive Bayes\n",
      "Accuracy: 0.8218\n",
      "Precision: 0.7104\n",
      "Recall: 0.9570\n",
      "F1 Score: 0.8131\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "from sklearn import metrics\n",
    "\n",
    "# Data preparation\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\"\n",
    "data = pd.read_csv(url, header=None)\n",
    "X = data.iloc[:, :-1]\n",
    "y = data.iloc[:, -1]\n",
    "\n",
    "# Implementation\n",
    "classifiers = {'Bernoulli Naive Bayes': BernoulliNB(),\n",
    "               'Multinomial Naive Bayes': MultinomialNB(),\n",
    "               'Gaussian Naive Bayes': GaussianNB()}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    # 10-fold cross-validation\n",
    "    scores = cross_val_score(clf, X, y, cv=10)\n",
    "    # Performance metrics\n",
    "    accuracy = scores.mean()\n",
    "    precision = cross_val_score(clf, X, y, cv=10, scoring='precision').mean()\n",
    "    recall = cross_val_score(clf, X, y, cv=10, scoring='recall').mean()\n",
    "    f1 = cross_val_score(clf, X, y, cv=10, scoring='f1').mean()\n",
    "    results[name] = {'Accuracy': accuracy,\n",
    "                     'Precision': precision,\n",
    "                     'Recall': recall,\n",
    "                     'F1 Score': f1}\n",
    "\n",
    "# Results\n",
    "for name, metrics in results.items():\n",
    "    print(f'Classifier: {name}')\n",
    "    for metric, value in metrics.items():\n",
    "        print(f'{metric}: {value:.4f}')\n",
    "    print('-------------------------------------------')\n",
    "\n",
    "# Discussion\n",
    "# Discuss the results obtained and compare the performance of each variant of Naive Bayes.\n",
    "# Analyze which variant performed the best and discuss possible reasons for its performance.\n",
    "# Also, mention any limitations of Naive Bayes observed during the evaluation.\n",
    "\n",
    "# Conclusion\n",
    "# Summarize the findings, including which Naive Bayes variant performed the best and why.\n",
    "# Provide suggestions for future work, such as exploring feature engineering techniques or trying other classifiers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff697a84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
