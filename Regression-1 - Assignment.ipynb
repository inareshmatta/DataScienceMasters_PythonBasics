{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75b89dca",
   "metadata": {},
   "source": [
    "## Regression-1 - Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9f3e4f",
   "metadata": {},
   "source": [
    "### Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an example of each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d825250a",
   "metadata": {},
   "source": [
    "Simple linear regression and multiple linear regression are both statistical techniques used to model the relationship between one or more independent variables (features) and a dependent variable (target). Here's how they differ:\n",
    "\n",
    "### Simple Linear Regression:\n",
    "- **Definition**: Simple linear regression involves modeling the relationship between a single independent variable \\( X \\) and a dependent variable $( y )$.\n",
    "- **Equation**: The equation of a simple linear regression model is represented as:\n",
    "  $[ y = \\beta_0 + \\beta_1 X + \\epsilon ]$\n",
    "  where:\n",
    "  - $( y )$ is the dependent variable,\n",
    "  - $( X )$ is the independent variable,\n",
    "  - $( \\beta_0)$ is the intercept (constant term),\n",
    "  - $( \\beta_1 )$ is the coefficient of the independent variable $( X )$,\n",
    "  - $( \\epsilon )$ represents the error term.\n",
    "- **Example**: Predicting house prices based on the square footage of the house. Here, square footage (X) is the independent variable, and house price (y) is the dependent variable.\n",
    "\n",
    "### Multiple Linear Regression:\n",
    "- **Definition**: Multiple linear regression involves modeling the relationship between two or more independent variables $( X_1, X_2, \\ldots, X_n )$ and a dependent variable $( y )$.\n",
    "- **Equation**: The equation of a multiple linear regression model is represented as:\n",
    "  $[ y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\ldots + \\beta_n X_n + \\epsilon ]$\n",
    "  where:\n",
    "  - $( y )$ is the dependent variable,\n",
    "  - $( X_1, X_2, \\ldots, X_n )$ are the independent variables,\n",
    "  - $( \\beta_0 )$ is the intercept (constant term),\n",
    "  - $( \\beta_1, \\beta_2, \\ldots, \\beta_n )$ are the coefficients of the independent variables,\n",
    "  - $( \\epsilon )$ represents the error term.\n",
    "- **Example**: Predicting the price of a house based on multiple factors such as square footage, number of bedrooms, number of bathrooms, and location. Here, square footage, number of bedrooms, number of bathrooms, and location are independent variables, and house price is the dependent variable.\n",
    "\n",
    "### Example:\n",
    "Let's consider an example of predicting students' exam scores:\n",
    "- Simple Linear Regression: Predicting exam scores based on the number of hours spent studying.\n",
    "- Multiple Linear Regression: Predicting exam scores based on the number of hours spent studying and the number of practice tests taken.\n",
    "\n",
    "In simple linear regression, we have one independent variable (hours spent studying) affecting the dependent variable (exam scores). In multiple linear regression, we include additional independent variables (such as the number of practice tests taken) that may also influence exam scores, making the model more complex but potentially more accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29409f76",
   "metadata": {},
   "source": [
    "### Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in a given dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fea5846",
   "metadata": {},
   "source": [
    "Linear regression relies on several key assumptions to be valid. Here are the main assumptions of linear regression:\n",
    "\n",
    "1. **Linearity**: The relationship between the independent variables and the dependent variable should be linear. This means that the change in the dependent variable is proportional to the change in the independent variable(s).\n",
    "\n",
    "2. **Independence of Errors**: The errors (residuals) should be independent of each other. In other words, the residuals should not be correlated with each other.\n",
    "\n",
    "3. **Homoscedasticity (Constant Variance of Residuals)**: The variance of the residuals should be constant across all levels of the independent variables. This implies that the spread of residuals should be uniform as the predicted values change.\n",
    "\n",
    "4. **Normality of Residuals**: The residuals should follow a normal distribution. This assumption is about the distribution of the errors, not the distribution of the dependent variable or the independent variables.\n",
    "\n",
    "5. **No Perfect Multicollinearity**: There should be no perfect linear relationship between the independent variables. In other words, the independent variables should not be highly correlated with each other.\n",
    "\n",
    "### Checking Assumptions:\n",
    "\n",
    "1. **Residuals vs. Fitted Values Plot**: Plot the residuals against the fitted (predicted) values. This helps to assess linearity and identify potential outliers.\n",
    "\n",
    "2. **Residuals Autocorrelation Plot**: Check for autocorrelation in the residuals using a correlogram. Autocorrelation indicates that the assumption of independence of errors is violated.\n",
    "\n",
    "3. **Residuals vs. Predictor Variables Plot**: Plot the residuals against each independent variable to detect patterns or nonlinearity.\n",
    "\n",
    "4. **Histogram or Q-Q Plot of Residuals**: Examine the distribution of residuals to assess normality. Histograms and quantile-quantile (Q-Q) plots can help with this.\n",
    "\n",
    "5. **Variance Inflation Factor (VIF)**: Calculate the VIF for each independent variable to detect multicollinearity. VIF values above 10 indicate high multicollinearity.\n",
    "\n",
    "6. **Durbin-Watson Statistic**: This tests for autocorrelation in the residuals. A value between 1 and 3 is generally acceptable, with values close to 2 indicating no autocorrelation.\n",
    "\n",
    "7. **Cook's Distance**: This measures the influence of each observation on the regression coefficients. High Cook's distances indicate influential outliers.\n",
    "\n",
    "By assessing these diagnostics and plots, you can gain insight into whether the assumptions of linear regression hold in your dataset. If the assumptions are violated, corrective actions may be needed, such as transforming variables, removing outliers, or considering alternative modeling techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfb13f0",
   "metadata": {},
   "source": [
    "### Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using a real-world scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92884170",
   "metadata": {},
   "source": [
    "In a linear regression model of the form $( Y = \\beta_0 + \\beta_1 X + \\epsilon )$, where $( Y )$ is the dependent variable, $( X )$ is the independent variable, $( \\beta_0)$ is the intercept, $( \\beta_1 )$ is the slope coefficient, and $( \\epsilon )$ is the error term, the slope and intercept have specific interpretations:\n",
    "\n",
    "1. **Intercept $( \\beta_0 )$**: The intercept represents the predicted value of the dependent variable when the independent variable is zero. It is the value of $( Y )$ when $( X )$ equals zero. In some cases, the intercept may not have a meaningful interpretation, especially if the range of the independent variable does not include zero.\n",
    "\n",
    "2. **Slope $( \\beta_1 )$**: The slope represents the change in the dependent variable for a one-unit change in the independent variable. It indicates the rate of change in $( Y )$ with respect to changes in $( X )$. A positive slope suggests a positive relationship between $( X )$ and $( Y )$, while a negative slope suggests a negative relationship.\n",
    "\n",
    "**Example:**\n",
    "Let's consider a real-world scenario where we want to predict the price of houses based on their size (in square feet). We collect data on house sizes and their corresponding prices and fit a linear regression model.\n",
    "\n",
    "- **Intercept**: The intercept represents the base price of a house with zero square feet. In reality, a house cannot have zero square feet, so the intercept might not have a direct interpretation in this context. It's often used to ensure that the line fits the data well.\n",
    "\n",
    "- **Slope**: The slope represents the change in the house price for each additional square foot. For example, if the slope coefficient is $100, it means that, on average, each additional square foot increases the house price by $100.\n",
    "\n",
    "So, in this scenario, if the regression equation is $( \\text{Price} = 50000 + 100 \\times \\text{Size} )$, the interpretation would be that the base price of a house (without any square feet) is $50,000, and for every additional square foot, the price increases by $100.\n",
    "\n",
    "Interpreting the slope and intercept allows us to understand how changes in the independent variable (house size) impact the dependent variable (house price) in the context of the linear regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42812a52",
   "metadata": {},
   "source": [
    "### Q4. Explain the concept of gradient descent. How is it used in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d443a9",
   "metadata": {},
   "source": [
    "Gradient descent is an optimization algorithm used to minimize the cost function of a machine learning model by adjusting its parameters iteratively. It's a fundamental technique used in various machine learning algorithms, especially in training models like linear regression, logistic regression, neural networks, and more.\n",
    "\n",
    "Here's how gradient descent works:\n",
    "\n",
    "1. **Initialization**: The algorithm starts by initializing the model parameters (weights) with random values or zeros.\n",
    "\n",
    "2. **Calculate the Cost Function**: The cost function measures how well the model performs on the training data. It quantifies the difference between the predicted values and the actual values. The goal of gradient descent is to minimize this cost function.\n",
    "\n",
    "3. **Compute the Gradient**: The gradient of the cost function with respect to each parameter (weight) is calculated. The gradient indicates the direction of the steepest increase of the cost function. To minimize the cost function, we need to move in the opposite direction of the gradient.\n",
    "\n",
    "4. **Update Parameters**: The parameters are updated iteratively using the gradient. The update rule is defined as follows:\n",
    "   $[ \\theta_{\\text{new}} = \\theta_{\\text{old}} - \\alpha \\cdot \\nabla J(\\theta_{\\text{old}}) ]$\n",
    "   where:\n",
    "   - $( \\theta_{\\text{new}} )$ is the new parameter value.\n",
    "   - $( \\theta_{\\text{old}} )$ is the old parameter value.\n",
    "   - $( \\alpha )$ is the learning rate, a hyperparameter that controls the size of the step taken in the direction of the gradient.\n",
    "   - $( \\nabla J(\\theta_{\\text{old}}) )$ is the gradient of the cost function with respect to the parameters.\n",
    "\n",
    "5. **Repeat**: Steps 2 to 4 are repeated until the algorithm converges to the minimum of the cost function or reaches a predefined number of iterations.\n",
    "\n",
    "Gradient descent can have different variants based on how it updates the parameters and the amount of data it uses to compute the gradient (batch gradient descent, stochastic gradient descent, mini-batch gradient descent).\n",
    "\n",
    "In summary, gradient descent is a fundamental optimization algorithm used in machine learning to train models by iteratively updating parameters to minimize the cost function and improve the model's performance on the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e323b0",
   "metadata": {},
   "source": [
    "### Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fbd67f",
   "metadata": {},
   "source": [
    "Multiple linear regression is an extension of simple linear regression that allows us to model the relationship between multiple independent variables (features) and a single dependent variable (target). In multiple linear regression, the model equation is:\n",
    "\n",
    "$[ Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\ldots + \\beta_n X_n + \\epsilon ]$\n",
    "\n",
    "where:\n",
    "- $( Y )$ is the dependent variable (target).\n",
    "- $( \\beta_0)$ is the y-intercept (constant).\n",
    "- $( \\beta_1, \\beta_2, \\ldots, \\beta_n )$ are the coefficients (parameters) of the independent variables $( X_1, X_2, \\ldots, X_n )$, respectively.\n",
    "- $( X_1, X_2, \\ldots, X_n )$ are the independent variables (features).\n",
    "- $( \\epsilon )$ is the error term.\n",
    "\n",
    "The main differences between simple linear regression and multiple linear regression are:\n",
    "\n",
    "1. **Number of Variables**: Simple linear regression involves only one independent variable, while multiple linear regression involves two or more independent variables.\n",
    "\n",
    "2. **Model Complexity**: Multiple linear regression models are more complex than simple linear regression models because they capture the relationships between multiple independent variables and the dependent variable.\n",
    "\n",
    "3. **Coefficient Interpretation**: In simple linear regression, there is only one coefficient (slope) to interpret. In multiple linear regression, each coefficient represents the change in the dependent variable when the corresponding independent variable changes, holding all other variables constant.\n",
    "\n",
    "4. **Model Evaluation**: Model evaluation in multiple linear regression involves assessing the overall fit of the model, the significance of individual predictors, multicollinearity (correlation between independent variables), and other assumptions.\n",
    "\n",
    "5. **Overfitting**: With multiple linear regression, there is a higher risk of overfitting, especially when including many irrelevant or highly correlated independent variables.\n",
    "\n",
    "In summary, multiple linear regression allows us to model the relationship between multiple independent variables and a dependent variable, providing a more comprehensive understanding of the factors influencing the target variable compared to simple linear regression. However, it requires careful consideration of model assumptions and variable selection to ensure the model's reliability and interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4db13c2",
   "metadata": {},
   "source": [
    "### Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and address this issue?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea20e7c6",
   "metadata": {},
   "source": [
    "Multicollinearity in multiple linear regression occurs when two or more independent variables in the model are highly correlated with each other. This correlation can cause issues in the regression analysis, including unstable parameter estimates, inflated standard errors, and difficulty in interpreting the coefficients.\n",
    "\n",
    "### Effects of Multicollinearity:\n",
    "1. **Unstable Coefficients**: Multicollinearity can lead to unstable estimates of the coefficients. Small changes in the data can result in large changes in the estimated coefficients.\n",
    "2. **Inflated Standard Errors**: Multicollinearity inflates the standard errors of the coefficients, making the estimates less precise and reducing the statistical significance of the predictors.\n",
    "3. **Difficulty in Interpretation**: Highly correlated predictors make it challenging to interpret the individual effects of each variable on the dependent variable.\n",
    "\n",
    "### Detection of Multicollinearity:\n",
    "1. **Correlation Matrix**: Calculate the correlation matrix between the independent variables. High correlation coefficients (close to Â±1) indicate potential multicollinearity.\n",
    "2. **Variance Inflation Factor (VIF)**: VIF measures how much the variance of the estimated regression coefficients is inflated due to multicollinearity. VIF values greater than 5 or 10 are often considered problematic.\n",
    "\n",
    "### Addressing Multicollinearity:\n",
    "1. **Variable Selection**: Remove one of the highly correlated variables from the model. Choose the variable that is less theoretically relevant or has a weaker correlation with the dependent variable.\n",
    "2. **Principal Component Analysis (PCA)**: PCA can be used to create orthogonal components that capture most of the variation in the data while avoiding multicollinearity.\n",
    "3. **Ridge Regression**: Ridge regression adds a penalty term to the regression coefficients, which reduces their magnitude and helps to stabilize the estimates in the presence of multicollinearity.\n",
    "4. **Combine Variables**: Create new variables by combining the highly correlated variables or use domain knowledge to create composite variables that capture the shared variation.\n",
    "\n",
    "### Summary:\n",
    "Multicollinearity is a common issue in multiple linear regression that can undermine the reliability and interpretability of the model. Detecting and addressing multicollinearity is essential for building robust regression models that provide accurate estimates of the relationships between the independent and dependent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07dd9ef",
   "metadata": {},
   "source": [
    "### Q7. Describe the polynomial regression model. How is it different from linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e0c123",
   "metadata": {},
   "source": [
    "Polynomial regression is a form of regression analysis in which the relationship between the independent variable $( x )$ and the dependent variable $( y )$ is modeled as an $( n )$-th degree polynomial. Unlike linear regression, which assumes a linear relationship between the independent and dependent variables, polynomial regression allows for more complex and nonlinear relationships to be captured.\n",
    "\n",
    "### Differences from Linear Regression:\n",
    "1. **Model Complexity**: In polynomial regression, the relationship between the independent and dependent variables is modeled using a polynomial function of degree $( n )$. This allows for more flexibility in capturing nonlinear patterns in the data compared to linear regression, which assumes a straight-line relationship.\n",
    "2. **Degree of the Polynomial**: In polynomial regression, the degree of the polynomial (i.e., the highest power of $( x )$ is a parameter that can be specified based on the complexity of the relationship being modeled. Higher-degree polynomials can capture more intricate patterns but may also be prone to overfitting.\n",
    "3. **Curve Fitting**: Polynomial regression fits a curve to the data, whereas linear regression fits a straight line. This allows polynomial regression to capture curvature and complex patterns that linear regression cannot.\n",
    "4. **Interpretability**: Polynomial regression models can be less interpretable than linear regression models, especially for higher-degree polynomials. Interpreting the coefficients of polynomial terms becomes more challenging as the degree of the polynomial increases.\n",
    "5. **Risk of Overfitting**: Polynomial regression models with high-degree polynomials are susceptible to overfitting, especially when the model captures noise in the data rather than the underlying patterns.\n",
    "\n",
    "### Example:\n",
    "Consider a scenario where we are modeling the relationship between the age of a car (in years) and its resale value (in dollars). While linear regression would assume a constant rate of depreciation over time (a straight line), polynomial regression could capture more nuanced patterns such as rapid depreciation in the early years followed by a slower decline.\n",
    "\n",
    "### Summary:\n",
    "Polynomial regression is a flexible technique that can capture nonlinear relationships between variables by fitting a polynomial function to the data. It extends the capabilities of linear regression by allowing for more complex relationships to be modeled, but it also requires careful consideration of model complexity and the risk of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578f7f88",
   "metadata": {},
   "source": [
    "### Q8. What are the advantages and disadvantages of polynomial regression compared to linear regression? In what situations would you prefer to use polynomial regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9311aec8",
   "metadata": {},
   "source": [
    "Polynomial regression offers several advantages and disadvantages compared to linear regression, and the choice between the two depends on the specific characteristics of the dataset and the underlying relationship between the variables. Here are the advantages and disadvantages of polynomial regression:\n",
    "\n",
    "### Advantages of Polynomial Regression:\n",
    "1. **Captures Nonlinear Relationships**: Polynomial regression can capture nonlinear relationships between the independent and dependent variables. It can model complex patterns that linear regression cannot, making it suitable for datasets with nonlinear trends.\n",
    "2. **Flexibility in Model Fitting**: By allowing for higher-degree polynomials, polynomial regression offers flexibility in fitting the data. It can accommodate curves of different shapes and degrees of complexity.\n",
    "3. **No Assumption of Linearity**: Unlike linear regression, polynomial regression does not assume a linear relationship between the variables. This makes it more versatile for modeling real-world data where linear relationships may not hold.\n",
    "\n",
    "### Disadvantages of Polynomial Regression:\n",
    "1. **Increased Model Complexity**: Polynomial regression models can become complex, especially with higher-degree polynomials. As the degree of the polynomial increases, the model may overfit the training data and generalize poorly to new data.\n",
    "2. **Interpretability Challenges**: Interpretation of polynomial regression models becomes more challenging as the degree of the polynomial increases. Coefficients of higher-order polynomial terms may not have straightforward interpretations.\n",
    "3. **Risk of Overfitting**: Polynomial regression models with high-degree polynomials are prone to overfitting, especially when the model captures noise rather than the underlying patterns in the data.\n",
    "\n",
    "### When to Use Polynomial Regression:\n",
    "- **Nonlinear Relationships**: When there is evidence of a nonlinear relationship between the independent and dependent variables, polynomial regression can be a suitable choice.\n",
    "- **Curvilinear Patterns**: Polynomial regression is appropriate when the relationship between the variables follows a curvilinear pattern that cannot be adequately captured by a straight line.\n",
    "- **Flexibility in Model Complexity**: When flexibility in model complexity is desired and there is a need to fit complex patterns in the data, polynomial regression offers more flexibility than linear regression.\n",
    "\n",
    "### Situations to Be Cautious:\n",
    "- **Overfitting**: Care should be taken to avoid overfitting when using polynomial regression, especially with high-degree polynomials. Regularization techniques such as ridge regression or cross-validation can help mitigate overfitting.\n",
    "- **Interpretation Challenges**: Interpretation of polynomial regression models can be challenging, particularly with higher-order polynomials. Simple models are often preferred when interpretability is important.\n",
    "\n",
    "In summary, polynomial regression is a powerful tool for capturing nonlinear relationships in the data, but it requires careful consideration of model complexity, overfitting, and interpretation challenges. It is best suited for situations where linear regression is inadequate to capture the underlying patterns in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb405076",
   "metadata": {},
   "source": [
    "Here's a table comparing three regression forms: Simple Linear Regression, Multiple Linear Regression, and Polynomial Regression, along with explanations of when to use and when not to use each, along with example scenarios and sample data:\n",
    "\n",
    "| Regression Form         | When to Use                                   | When Not to Use                               | Example Scenario and Data                                  |\n",
    "|-------------------------|-----------------------------------------------|-----------------------------------------------|------------------------------------------------------------|\n",
    "| Simple Linear Regression| - When there is a linear relationship between the independent and dependent variables.<br>- When the relationship between variables can be adequately represented by a straight line.<br>- When the number of predictors is low.<br> | - When the relationship between variables is not linear.<br>- When there are multiple predictors and the relationship is complex.<br>- When assumptions of linearity, homoscedasticity, and independence are violated.| Suppose we want to predict students' exam scores based on the number of hours they study. Here's some sample data:<br> Hours Studied (X) | Exam Score (Y)<br> 5    | 50<br> 6    | 55<br> 7    | 60<br> 8    | 65<br> 9    | 70 |\n",
    "| Multiple Linear Regression| - When there are multiple predictors influencing the dependent variable.<br>- When the relationship between predictors and the dependent variable is linear.<br>- When there is a need to understand the unique contribution of each predictor while controlling for others.<br> | - When there is multicollinearity among predictors, which can lead to unstable coefficient estimates.<br>- When the assumptions of linearity, homoscedasticity, and independence are not met.<br>- When the sample size is small relative to the number of predictors, leading to overfitting.<br> | Suppose we want to predict the price of a house based on its size (in square feet), the number of bedrooms, and the neighborhood. Here's some sample data:<br> Size (X1) | Bedrooms (X2) | Neighborhood (X3) | Price (Y)<br> 1500 | 3 | Suburban | 250,000<br> 2000 | 4 | Urban | 350,000<br> 1800 | 3 | Rural | 200,000 |\n",
    "| Polynomial Regression   | - When the relationship between the independent and dependent variables is nonlinear.<br>- When the data exhibits a curvilinear pattern that cannot be captured by simple linear or multiple linear regression.<br>- When there is evidence of higher-order polynomial effects in the data.<br> | - When the degree of the polynomial is too high, leading to overfitting.<br>- When the relationship between variables is linear, as polynomial regression can introduce unnecessary complexity.<br>- When interpretation of the model becomes difficult with higher-degree polynomials.<br>| Suppose we want to predict the sales of a product based on the temperature outside. Here's some sample data:<br> Temperature (X) | Sales (Y)<br> 10 | 100<br> 20 | 150<br> 30 | 200<br> 40 | 180<br> 50 | 120 |\n",
    "\n",
    "In the example scenarios provided, each regression form is applied to a specific dataset to predict an outcome variable based on predictor variables. The choice of regression technique depends on the nature of the relationship between variables and the assumptions underlying each regression method. By considering the characteristics of the data and the goals of the analysis, one can determine the most appropriate regression approach for the given problem.\n",
    "\n",
    "**Example Scenarios:**\n",
    "\n",
    "1. **Simple Linear Regression**: Suppose you want to predict the price of a house based on its size. Simple linear regression is appropriate when you expect the price to increase linearly with the size of the house. However, it would not be suitable if the relationship between size and price is more complex, such as if larger houses become disproportionately more expensive.\n",
    "\n",
    "2. **Multiple Linear Regression**: Consider a scenario where you want to predict the salary of employees based on their years of experience, level of education, and job position. Multiple linear regression is suitable here as there are multiple predictors influencing salary, and the relationship is expected to be linear.\n",
    "\n",
    "3. **Polynomial Regression**: Suppose you are analyzing the relationship between temperature and ice cream sales. As temperature increases, ice cream sales may initially rise, peak, and then decline. Polynomial regression can capture this nonlinear relationship better than simple linear regression. However, it may not be suitable if the relationship is strictly linear or if the degree of the polynomial leads to overfitting.\n",
    "\n",
    "In summary, the choice of regression method depends on the nature of the data, the relationship between variables, and the underlying assumptions. It's essential to consider these factors carefully when selecting the appropriate regression technique for a given problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae1c764",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
