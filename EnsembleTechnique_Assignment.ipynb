{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74d17502",
   "metadata": {},
   "source": [
    "# Ensemble Techniques And Its Types-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1a1672",
   "metadata": {},
   "source": [
    "Q1. What is an ensemble technique in machine learning?  \n",
    "Q2. Why are ensemble techniques used in machine learning?  \n",
    "Q3. What is bagging?   \n",
    "Q4. What is boosting?  \n",
    "Q5. What are the benefits of using ensemble techniques?   \n",
    "Q6. Are ensemble techniques always better than individual models?   \n",
    "Q7. How is the confidence interval calculated using bootstrap?   \n",
    "Q8. How does bootstrap work and What are the steps involved in bootstrap?   \n",
    "\n",
    "\n",
    "Q9. A researcher wants to estimate the mean height of a population of trees. They measure the height of a\n",
    "sample of 50 trees and obtain a mean height of 15 meters and a standard deviation of 2 meters. Use\n",
    "bootstrap to estimate the 95% confidence interval for the population mean height.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aeece99",
   "metadata": {},
   "source": [
    "### Q1. What is an ensemble technique in machine learning? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5af2df",
   "metadata": {},
   "source": [
    "An ensemble technique in machine learning is a method that combines predictions from multiple individual models to produce a final prediction. The main idea behind ensemble techniques is to leverage the collective wisdom of diverse models to improve overall predictive performance. Ensemble methods are widely used because they often yield better results compared to single models by reducing variance, bias, or both.\n",
    "\n",
    "There are several types of ensemble techniques, including:\n",
    "\n",
    "1. **Voting**: In this method, predictions from multiple models (e.g., classifiers or regressors) are combined using a simple majority vote (for classification) or averaging (for regression).\n",
    "\n",
    "2. **Bagging (Bootstrap Aggregating)**: Bagging involves training multiple instances of the same base model on different subsets of the training data, often using bootstrap sampling with replacement. The final prediction is obtained by averaging (for regression) or voting (for classification) the predictions of the individual models.\n",
    "\n",
    "3. **Boosting**: Boosting is an iterative ensemble technique where models are trained sequentially, with each subsequent model focusing on the instances that the previous models misclassified. The final prediction is obtained by combining the predictions of all models, typically using a weighted sum.\n",
    "\n",
    "4. **Stacking (Stacked Generalization)**: Stacking involves training multiple diverse base models and using their predictions as features to train a meta-model (also called a blender or a combiner). The meta-model learns how to best combine the predictions of the base models to make the final prediction.\n",
    "\n",
    "5. **Random Forest**: Random Forest is a specific ensemble method based on bagging. It constructs multiple decision trees, each trained on a random subset of the features and using bootstrap sampling. The final prediction is obtained by averaging (for regression) or voting (for classification) the predictions of all trees.\n",
    "\n",
    "Ensemble techniques are powerful tools in machine learning because they can mitigate overfitting, increase model robustness, and improve predictive performance, especially in complex and noisy datasets. They are widely used in various applications, including classification, regression, and anomaly detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31801693",
   "metadata": {},
   "source": [
    "### Q2. Why are ensemble techniques used in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4091f572",
   "metadata": {},
   "source": [
    "Ensemble techniques are used in machine learning for several reasons, as they offer various benefits and advantages over single models. Here are some key reasons why ensemble techniques are widely employed:\n",
    "\n",
    "1. **Improved Predictive Performance**: Ensemble methods often lead to better predictive performance compared to individual models. By combining predictions from multiple models, ensemble techniques can reduce errors and variance, leading to more accurate and robust predictions.\n",
    "\n",
    "2. **Reduction of Overfitting**: Ensemble techniques help mitigate overfitting, which occurs when a model learns to memorize the training data instead of generalizing well to new, unseen data. By combining multiple models trained on different subsets of the data or with different algorithms, ensemble methods can reduce overfitting and improve generalization.\n",
    "\n",
    "3. **Robustness to Noise**: Ensemble methods are more robust to noise and outliers in the data compared to single models. Since predictions are aggregated from multiple models, outliers or errors from individual models are often mitigated or canceled out, leading to more stable and reliable predictions.\n",
    "\n",
    "4. **Handling Complexity**: Ensemble techniques are effective in capturing complex relationships and patterns in the data. By combining diverse models that may capture different aspects of the underlying data distribution, ensemble methods can better handle complex, high-dimensional datasets and non-linear relationships.\n",
    "\n",
    "5. **Enhanced Stability**: Ensemble techniques provide stability to the model's predictions. Since predictions are averaged or combined from multiple models, small changes in the training data or model parameters are less likely to result in significant changes in the final prediction, making ensemble methods more robust and stable.\n",
    "\n",
    "6. **Versatility**: Ensemble techniques are versatile and can be applied to a wide range of machine learning tasks, including classification, regression, and anomaly detection. They can be used with various base models and algorithms, making them adaptable to different types of datasets and problem domains.\n",
    "\n",
    "7. **Interpretability**: In some cases, ensemble techniques can provide insights into the underlying data and model behavior. For example, feature importance scores from ensemble methods like Random Forest can help identify the most informative features in the dataset.\n",
    "\n",
    "Overall, ensemble techniques are used in machine learning to improve predictive performance, increase model robustness, and handle complex and noisy datasets effectively. They are an essential tool in the machine learning practitioner's toolkit and are widely employed in both academic research and practical applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d334d9",
   "metadata": {},
   "source": [
    "### Q3. What is bagging?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89697233",
   "metadata": {},
   "source": [
    "Bagging, short for Bootstrap Aggregating, is an ensemble technique in machine learning that aims to improve the stability and accuracy of machine learning algorithms, especially decision trees. It involves training multiple instances of the same base model on different subsets of the training data, often using bootstrap sampling with replacement.\n",
    "\n",
    "Here's a more detailed explanation of bagging:\n",
    "\n",
    "1. **Bootstrap Sampling**: Bagging starts by creating multiple bootstrap samples from the original training dataset. Bootstrap sampling involves randomly sampling the training dataset with replacement to create new datasets of the same size as the original dataset. This process allows some instances to appear multiple times in the new datasets while others may not appear at all.\n",
    "\n",
    "2. **Base Model Training**: Once the bootstrap samples are created, a base model (e.g., decision tree) is trained on each bootstrap sample independently. Since each bootstrap sample is slightly different from the original dataset, each base model learns slightly different patterns and relationships in the data.\n",
    "\n",
    "3. **Combining Predictions**: After training the base models, predictions are made for new data points using each individual model. For classification tasks, the final prediction is typically obtained by aggregating the predictions of all base models through majority voting. For regression tasks, predictions are averaged across all base models.\n",
    "\n",
    "Bagging helps to reduce variance and overfitting by introducing diversity among the base models. Since each base model is trained on a different subset of the data, they may make different errors or focus on different aspects of the data. By averaging or combining the predictions of multiple base models, bagging produces a more robust and stable prediction, leading to better generalization performance on unseen data.\n",
    "\n",
    "The most popular implementation of bagging is the Random Forest algorithm, which uses bagging to train multiple decision trees and combines their predictions to make a final prediction. However, bagging can be applied to other base models as well, such as k-nearest neighbors, support vector machines, and neural networks, to improve their performance and stability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa8da8e",
   "metadata": {},
   "source": [
    "### Q4. What is boosting?\n",
    "\n",
    "Boosting is an ensemble learning technique in machine learning that combines multiple weak learners (typically shallow decision trees or simple classifiers) sequentially to create a strong learner. The main idea behind boosting is to focus on training instances that are difficult to classify, assigning higher weights to them in subsequent iterations, thereby improving the model's performance over time.\n",
    "\n",
    "Here's how boosting works:\n",
    "\n",
    "1. **Sequential Training**: Boosting trains a sequence of weak learners (base models) sequentially, where each model focuses on the instances that were misclassified by the previous models. Initially, all instances are given equal weights.\n",
    "\n",
    "2. **Instance Weighting**: In each iteration, the weights of incorrectly classified instances are increased, while the weights of correctly classified instances are decreased. This allows subsequent weak learners to focus more on the difficult instances.\n",
    "\n",
    "3. **Model Combination**: The predictions of all weak learners are combined using a weighted sum, where the weights are determined based on the performance of each weak learner. Typically, a weighted majority vote is used for classification tasks, while a weighted average is used for regression tasks.\n",
    "\n",
    "4. **Iterative Improvement**: Boosting continues for a fixed number of iterations (boosting rounds) or until a predefined performance threshold is reached. Each subsequent weak learner is trained to correct the errors of the previous models, gradually improving the overall performance of the ensemble.\n",
    "\n",
    "Popular boosting algorithms include AdaBoost (Adaptive Boosting), Gradient Boosting Machines (GBM), XGBoost, and LightGBM. These algorithms differ in how they assign weights to instances and how they combine weak learners' predictions, but they all follow the general principle of iteratively improving the ensemble by focusing on difficult instances.\n",
    "\n",
    "Boosting is known for its ability to achieve high predictive performance, especially in structured/tabular data and classification problems with imbalanced classes. It is widely used in various applications, including classification, regression, and ranking.\n",
    "\n",
    "### Q5. What are the benefits of using ensemble techniques?\n",
    "\n",
    "Ensemble techniques offer several benefits in machine learning:\n",
    "\n",
    "1. **Improved Predictive Performance**: Ensemble methods often achieve higher accuracy and generalization performance compared to individual models by leveraging the wisdom of multiple diverse models.\n",
    "\n",
    "2. **Reduction of Overfitting**: Ensemble techniques help mitigate overfitting by combining predictions from multiple models, which reduces model variance and improves generalization to unseen data.\n",
    "\n",
    "3. **Robustness to Noise and Variability**: Ensemble methods are more robust to noisy data, outliers, and variability in the dataset because errors from individual models are often mitigated or canceled out when predictions are combined.\n",
    "\n",
    "4. **Handling Complex Relationships**: Ensemble techniques are effective in capturing complex relationships and patterns in the data by combining diverse models that may capture different aspects of the underlying data distribution.\n",
    "\n",
    "5. **Versatility and Adaptability**: Ensemble methods are versatile and can be applied to various types of machine learning tasks, including classification, regression, and anomaly detection. They can also be combined with different base models and algorithms, making them adaptable to different types of datasets and problem domains.\n",
    "\n",
    "6. **Interpretability and Explainability**: In some cases, ensemble techniques can provide insights into the underlying data and model behavior. For example, feature importance scores from ensemble methods like Random Forest can help identify the most informative features in the dataset.\n",
    "\n",
    "Overall, ensemble techniques are powerful tools in machine learning that can lead to improved predictive performance, robustness, and interpretability, making them widely used in both academic research and practical applications.\n",
    "\n",
    "### Q6. Are ensemble techniques always better than individual models?\n",
    "\n",
    "While ensemble techniques often outperform individual models in terms of predictive performance, they are not always guaranteed to be better in every scenario. Here are some factors to consider:\n",
    "\n",
    "1. **Data Quality**: Ensemble techniques may not provide significant benefits if the dataset is small, clean, and well-structured, and if there are no complex relationships to capture. In such cases, a single well-tuned model may perform comparably to an ensemble.\n",
    "\n",
    "2. **Model Diversity**: The effectiveness of ensemble techniques relies on the diversity among the base models. If the base models are similar or correlated, the ensemble may not perform significantly better than individual models.\n",
    "\n",
    "3. **Computational Cost**: Ensemble techniques typically require training and maintaining multiple models, which can be computationally expensive, especially for large datasets or complex models. In some cases, the computational cost may outweigh the benefits of using an ensemble.\n",
    "\n",
    "4. **Interpretability**: Ensemble models can be more complex and less interpretable than individual models, especially if the ensemble involves a large number of diverse models. In applications where interpretability is important, a single well-interpretable model may be preferred.\n",
    "\n",
    "5. **Overfitting**: Ensemble techniques can still suffer from overfitting if not properly tuned or if the base models are too complex. Overfitting can lead to poor generalization performance, even with ensembles.\n",
    "\n",
    "While ensemble techniques are a powerful tool in machine learning and are often the method of choice for improving predictive performance, it's essential to evaluate them carefully and consider the specific characteristics of the dataset and problem domain before deciding to use an ensemble.\n",
    "\n",
    "### Q7. How is the confidence interval calculated using bootstrap?\n",
    "\n",
    "The confidence interval (CI) is a statistical measure that quantifies the uncertainty or variability of a parameter estimate. In the context of bootstrap resampling, the confidence interval provides a range of plausible values for a population parameter (such as the mean or median) based on the distribution of estimates obtained from resampling the original dataset.\n",
    "\n",
    "Here's how the confidence interval is calculated using bootstrap:\n",
    "\n",
    "1. **Bootstrap Resampling**: Given an original dataset with n observations, bootstrap resampling involves randomly sampling n observations with replacement from the dataset to create a bootstrap sample. This process is repeated B times to generate multiple bootstrap samples.\n",
    "\n",
    "2. **Parameter Estimation**: For each bootstrap sample, the parameter of interest (e.g., mean, median, variance) is calculated. For example, if estimating the mean, the mean of each bootstrap sample is computed.\n",
    "\n",
    "3. **Confidence Interval Calculation**: Once parameter estimates are obtained for all bootstrap samples, the confidence interval is constructed based on the distribution of these estimates. The simplest approach is to use the percentile method, where the lower and upper bounds of\n",
    "\n",
    " the confidence interval are determined by the α/2 and 1-α/2 percentiles of the bootstrap estimates, respectively. Here, α represents the significance level (e.g., 0.05 for a 95% confidence interval).\n",
    "\n",
    "   For example, to calculate a 95% confidence interval using the percentile method:\n",
    "   - Lower bound = α/2 percentile of bootstrap estimates\n",
    "   - Upper bound = (1-α/2) percentile of bootstrap estimates\n",
    "\n",
    "4. **Estimation of Confidence Interval Width**: The width of the confidence interval provides information about the precision of the parameter estimate. A narrower confidence interval indicates greater precision, while a wider interval indicates more uncertainty.\n",
    "\n",
    "By repeating the bootstrap resampling process and calculating the confidence interval, we can obtain an estimate of the parameter's uncertainty and assess the stability of our estimation procedure. Bootstrap confidence intervals are widely used in statistical inference, hypothesis testing, and model evaluation due to their simplicity and flexibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb6e389",
   "metadata": {},
   "source": [
    "### Q8. How does bootstrap work and What are the steps involved in bootstrap?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0b43a4",
   "metadata": {},
   "source": [
    "Bootstrap is a resampling technique in statistics used to estimate the distribution of a statistic by repeatedly sampling with replacement from the observed data. It is a powerful method for estimating the sampling distribution of a statistic when the underlying distribution is unknown or when traditional parametric methods are not applicable.\n",
    "\n",
    "Here are the steps involved in bootstrap:\n",
    "\n",
    "1. **Original Sample**: Start with a dataset consisting of observed data points. This dataset could be a sample from a population or the entire population itself.\n",
    "\n",
    "2. **Resampling with Replacement**: Generate multiple bootstrap samples by randomly selecting data points from the original dataset with replacement. In bootstrap sampling, each data point has an equal chance of being selected for inclusion in the bootstrap sample, and each data point can be selected multiple times or not at all.\n",
    "\n",
    "3. **Sample Size**: Each bootstrap sample should have the same size as the original dataset. By sampling with replacement, some observations may be duplicated in the bootstrap sample, while others may not appear at all.\n",
    "\n",
    "4. **Estimation**: Calculate the statistic of interest (e.g., mean, median, variance) for each bootstrap sample. This statistic could be any function of the data, such as a parameter estimate or a model performance metric.\n",
    "\n",
    "5. **Repeat**: Repeat steps 2-4 a large number of times (typically thousands or tens of thousands) to generate multiple estimates of the statistic from the bootstrap samples.\n",
    "\n",
    "6. **Distribution Estimation**: Use the collection of bootstrap estimates to construct an empirical distribution of the statistic. This distribution provides information about the variability or uncertainty of the statistic and can be used to estimate confidence intervals or conduct hypothesis tests.\n",
    "\n",
    "7. **Statistical Inference**: Perform statistical inference based on the estimated distribution of the statistic. For example, calculate confidence intervals, conduct hypothesis tests, or compare different models or treatments.\n",
    "\n",
    "Bootstrap resampling allows us to approximate the sampling distribution of a statistic without making assumptions about the underlying population distribution. It provides a non-parametric method for estimating uncertainty and variability, making it widely used in hypothesis testing, confidence interval estimation, and model evaluation. Additionally, bootstrap can be applied to various types of data and statistical models, making it a versatile tool in statistical analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1aa187",
   "metadata": {},
   "source": [
    "### Q9. A researcher wants to estimate the mean height of a population of trees. They measure the height of a sample of 50 trees and obtain a mean height of 15 meters and a standard deviation of 2 meters. Use bootstrap to estimate the 95% confidence interval for the population mean height."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b46e8f",
   "metadata": {},
   "source": [
    "To estimate the 95% confidence interval for the population mean height using bootstrap, we can follow these steps:\n",
    "\n",
    "1. **Original Sample**: Start with the sample of 50 tree heights.\n",
    "\n",
    "2. **Bootstrap Resampling**: Generate multiple bootstrap samples by randomly selecting 50 tree heights from the original sample with replacement.\n",
    "\n",
    "3. **Calculate Mean**: For each bootstrap sample, calculate the mean height of the trees.\n",
    "\n",
    "4. **Repeat**: Repeat steps 2 and 3 a large number of times (e.g., 10,000 times) to create a distribution of bootstrap sample means.\n",
    "\n",
    "5. **Confidence Interval**: Determine the 2.5th and 97.5th percentiles of the bootstrap sample means to obtain the bounds of the 95% confidence interval.\n",
    "\n",
    "Let's perform these steps in Python:\n",
    "\n",
    "\n",
    "In this code:\n",
    "\n",
    "- We generate `num_bootstraps` bootstrap samples by randomly sampling from a normal distribution with mean `sample_mean` and standard deviation `sample_std`, representing the sample mean and standard deviation obtained from the original sample of tree heights.\n",
    "\n",
    "- For each bootstrap sample, we calculate the mean height of the trees and store it in the `bootstrap_means` list.\n",
    "\n",
    "- Finally, we calculate the 95% confidence interval for the population mean height by finding the 2.5th and 97.5th percentiles of the bootstrap sample means.\n",
    "\n",
    "This gives us an estimate of the 95% confidence interval for the population mean height of the trees based on the bootstrap resampling method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4519a99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95% Confidence Interval for Population Mean Height: [14.44078235 15.54881105]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "sample_mean = 15  # Mean height of the sample\n",
    "sample_std = 2    # Standard deviation of the sample\n",
    "sample_size = 50   # Size of the sample\n",
    "\n",
    "# Number of bootstrap samples\n",
    "num_bootstraps = 10000\n",
    "\n",
    "# Generate bootstrap samples\n",
    "bootstrap_means = []\n",
    "for _ in range(num_bootstraps):\n",
    "    # Generate bootstrap sample by resampling with replacement\n",
    "    bootstrap_sample = np.random.normal(loc=sample_mean, scale=sample_std, size=sample_size)\n",
    "    # Calculate mean of bootstrap sample\n",
    "    bootstrap_mean = np.mean(bootstrap_sample)\n",
    "    # Append bootstrap mean to list\n",
    "    bootstrap_means.append(bootstrap_mean)\n",
    "\n",
    "# Calculate 95% confidence interval\n",
    "confidence_interval = np.percentile(bootstrap_means, [2.5, 97.5])\n",
    "\n",
    "# Print confidence interval\n",
    "print(\"95% Confidence Interval for Population Mean Height:\", confidence_interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294e3ca7",
   "metadata": {},
   "source": [
    "# Ensemble Techniques And Its Types-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b5cee8",
   "metadata": {},
   "source": [
    "Q1. How does bagging reduce overfitting in decision trees?  \n",
    "Q2. What are the advantages and disadvantages of using different types of base learners in bagging?  \n",
    "Q3. How does the choice of base learner affect the bias-variance tradeoff in bagging?   \n",
    "Q4. Can bagging be used for both classification and regression tasks? How does it differ in each case?  \n",
    "Q5. What is the role of ensemble size in bagging? How many models should be included in the ensemble?  \n",
    "Q6. Can you provide an example of a real-world application of bagging in machine learning?  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5075a032",
   "metadata": {},
   "source": [
    "### Q1. How does bagging reduce overfitting in decision trees?  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21294f5f",
   "metadata": {},
   "source": [
    "Bagging, short for Bootstrap Aggregating, is an ensemble learning technique that aims to reduce overfitting in decision trees and other machine learning models. Here's how bagging helps mitigate overfitting specifically in the context of decision trees:\n",
    "\n",
    "1. **High Variance of Decision Trees**: Decision trees have a tendency to overfit the training data, especially when they are deep and complex. Overfitting occurs when the model captures noise and idiosyncrasies in the training data, leading to poor generalization performance on unseen data.\n",
    "\n",
    "2. **Bootstrap Sampling**: Bagging addresses overfitting by training multiple decision trees on different subsets of the training data. It achieves this by generating bootstrap samples, which are random samples drawn with replacement from the original training dataset. Since each bootstrap sample is likely to contain different instances and potentially different noise patterns, each decision tree trained on a bootstrap sample will learn slightly different patterns from the training data.\n",
    "\n",
    "3. **Reduced Variance Through Aggregation**: After training multiple decision trees on different bootstrap samples, bagging combines their predictions using averaging (for regression) or majority voting (for classification). By averaging or voting over multiple trees, bagging reduces the variance of the predictions compared to a single decision tree. This aggregation process helps smooth out the noise and idiosyncrasies captured by individual trees, leading to a more robust and less overfitting model.\n",
    "\n",
    "4. **Feature Randomization**: In addition to training each decision tree on a different bootstrap sample, bagging also introduces randomness by considering only a random subset of features at each split during the construction of each tree. This further reduces the correlation among the trees and helps prevent overfitting by discouraging them from memorizing specific features or patterns in the training data.\n",
    "\n",
    "5. **Out-of-Bag (OOB) Evaluation**: Another advantage of bagging is that it allows for out-of-bag (OOB) evaluation. Since each decision tree is trained on a bootstrap sample, there will be instances in the original dataset that were not included in the bootstrap sample used to train a particular tree. These out-of-bag instances can be used to evaluate the performance of the model without the need for a separate validation set, providing an estimate of the model's generalization performance.\n",
    "\n",
    "In summary, bagging reduces overfitting in decision trees by training multiple trees on different subsets of the training data, aggregating their predictions, and introducing randomness through feature selection and bootstrap sampling. By combining the predictions of multiple trees, bagging creates a more stable and robust model that generalizes better to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8440e8",
   "metadata": {},
   "source": [
    "### Q2. What are the advantages and disadvantages of using different types of base learners in bagging?  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36462be7",
   "metadata": {},
   "source": [
    "In bagging, the choice of base learners (individual models) can have a significant impact on the performance and behavior of the ensemble. Here are the advantages and disadvantages of using different types of base learners in bagging:\n",
    "\n",
    "1. **Decision Trees**:\n",
    "\n",
    "   Advantages:\n",
    "   - Decision trees are versatile and can handle both numerical and categorical features without requiring feature scaling or one-hot encoding.\n",
    "   - They are interpretable, allowing for easy understanding of the learned decision rules.\n",
    "   - Decision trees naturally capture nonlinear relationships and interactions between features.\n",
    "   \n",
    "   Disadvantages:\n",
    "   - Decision trees tend to have high variance and are prone to overfitting, especially when they are deep and complex.\n",
    "   - They may struggle with capturing complex patterns in the data, particularly when the decision boundaries are highly nonlinear or involve interactions between many features.\n",
    "\n",
    "2. **Random Forest (Ensemble of Decision Trees)**:\n",
    "\n",
    "   Advantages:\n",
    "   - Random Forest inherits the advantages of decision trees, such as versatility and interpretability.\n",
    "   - By aggregating predictions from multiple decision trees trained on different subsets of the data, Random Forest reduces overfitting and improves generalization performance.\n",
    "   - Random Forest automatically selects a random subset of features at each split, which helps decorrelate the trees and further reduce overfitting.\n",
    "\n",
    "   Disadvantages:\n",
    "   - Random Forest tends to sacrifice some interpretability compared to individual decision trees, as it involves combining predictions from multiple trees.\n",
    "   - Training and inference times can be higher compared to single decision trees, especially for large datasets or deep forests with many trees.\n",
    "\n",
    "3. **Other Base Learners (e.g., Linear Models, Support Vector Machines)**:\n",
    "\n",
    "   Advantages:\n",
    "   - Linear models and support vector machines (SVMs) can be more computationally efficient than decision trees, especially for high-dimensional data or large datasets.\n",
    "   - They may perform well in scenarios where the underlying relationships in the data are approximately linear or can be well-separated by hyperplanes in the feature space.\n",
    "\n",
    "   Disadvantages:\n",
    "   - Linear models and SVMs may struggle to capture complex nonlinear relationships in the data, especially when interactions between features are important.\n",
    "   - They may require careful feature engineering or preprocessing to perform well, such as feature scaling or transformation.\n",
    "   - Linear models and SVMs may be less interpretable compared to decision trees, especially when used in ensembles.\n",
    "\n",
    "In summary, the choice of base learners in bagging should consider the trade-offs between interpretability, computational efficiency, and the ability to capture complex patterns in the data. Decision trees and Random Forest are popular choices due to their flexibility, while linear models and SVMs may be suitable for specific types of data and problem domains. Ultimately, experimenting with different base learners and evaluating their performance empirically is often necessary to determine the most effective approach for a given task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1d977c",
   "metadata": {},
   "source": [
    "### Q3. How does the choice of base learner affect the bias-variance tradeoff in bagging?   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83847b82",
   "metadata": {},
   "source": [
    "The choice of base learner in bagging can significantly affect the bias-variance tradeoff, which is a fundamental concept in machine learning. Here's how the choice of base learner impacts the bias-variance tradeoff in bagging:\n",
    "\n",
    "1. **Bias**:\n",
    "   - Bias refers to the error introduced by the model's assumptions when approximating the true underlying relationship between features and target variables.\n",
    "   - The choice of base learner can influence the bias of the bagged ensemble. For example, decision trees and random forests tend to have higher bias compared to more flexible models like support vector machines or neural networks.\n",
    "   - Base learners with higher bias may struggle to capture complex patterns in the data, leading to underfitting if the model is too simple.\n",
    "\n",
    "2. **Variance**:\n",
    "   - Variance refers to the model's sensitivity to fluctuations in the training data. Models with high variance are more susceptible to overfitting, capturing noise and idiosyncrasies in the training data.\n",
    "   - Bagging aims to reduce variance by training multiple base learners on different subsets of the training data and aggregating their predictions.\n",
    "   - The choice of base learner can affect the variance of the bagged ensemble. Models with higher variance, such as decision trees, can benefit more from bagging compared to models with lower variance.\n",
    "\n",
    "3. **Tradeoff**:\n",
    "   - The bias-variance tradeoff in bagging involves balancing the bias reduction achieved by averaging or voting over multiple base learners with potentially different biases against the increase in variance due to diversity among the base learners.\n",
    "   - If the base learners are too simple (high bias), the bagged ensemble may still suffer from underfitting, as it may not be able to capture complex patterns in the data.\n",
    "   - Conversely, if the base learners are too complex (low bias), the bagged ensemble may suffer from overfitting if the base learners are highly correlated, as they may all capture the same patterns and noise in the data.\n",
    "\n",
    "4. **Impact of Base Learner Choice**:\n",
    "   - The choice of base learner affects the bias-variance tradeoff by influencing the individual bias and variance of the base learners and their correlation within the ensemble.\n",
    "   - For example, using decision trees as base learners in bagging tends to reduce bias but may not reduce variance as effectively if the trees are highly correlated.\n",
    "   - On the other hand, using linear models or support vector machines as base learners may reduce variance but may introduce higher bias if the underlying relationships in the data are nonlinear.\n",
    "\n",
    "In summary, the choice of base learner in bagging affects the bias-variance tradeoff by influencing the bias and variance of the individual base learners and their correlation within the ensemble. Understanding this tradeoff is crucial for selecting an appropriate base learner and optimizing the performance of the bagged ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3feb84a5",
   "metadata": {},
   "source": [
    "### Q4. Can bagging be used for both classification and regression tasks? How does it differ in each case?  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed28956",
   "metadata": {},
   "source": [
    "Yes, bagging can be used for both classification and regression tasks. However, there are some differences in how bagging is applied and its impact in each case:\n",
    "\n",
    "1. **Classification**:\n",
    "   - In classification tasks, bagging involves training multiple base classifiers (e.g., decision trees, logistic regression models) on different bootstrap samples of the training data and aggregating their predictions using majority voting.\n",
    "   - The final prediction of the bagged ensemble is determined by a majority vote of the predictions from individual base classifiers.\n",
    "   - Bagging in classification aims to reduce variance, improve generalization performance, and make the classifier more robust to noisy data or overfitting.\n",
    "   - The predicted class labels may be discrete and categorical.\n",
    "\n",
    "2. **Regression**:\n",
    "   - In regression tasks, bagging involves training multiple base regression models (e.g., decision trees, linear regression models) on different bootstrap samples of the training data and aggregating their predictions using averaging.\n",
    "   - The final prediction of the bagged ensemble is determined by averaging the predictions from individual base models.\n",
    "   - Bagging in regression aims to reduce variance, smooth out the predictions, and improve the stability of the regression model.\n",
    "   - The predicted values may be continuous and numeric.\n",
    "\n",
    "**Differences**:\n",
    "- In classification, bagging typically uses majority voting to combine the predictions of individual classifiers, while in regression, bagging typically uses averaging to combine the predictions of individual regression models.\n",
    "- The nature of the predicted output differs between classification and regression tasks. In classification, the output is categorical (class labels), while in regression, the output is continuous (numeric values).\n",
    "- The evaluation metrics used to assess the performance of bagged classifiers and regressors may differ. For classification, metrics such as accuracy, precision, recall, and F1-score are commonly used, while for regression, metrics such as mean squared error (MSE), mean absolute error (MAE), and R-squared are commonly used.\n",
    "\n",
    "In summary, while bagging can be applied to both classification and regression tasks, there are differences in how it is applied and its impact in each case. Bagging aims to reduce variance, improve generalization performance, and make the model more robust to noise and overfitting, regardless of the task at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94211bbe",
   "metadata": {},
   "source": [
    "### Q5. What is the role of ensemble size in bagging? How many models should be included in the ensemble?  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21686b36",
   "metadata": {},
   "source": [
    "The ensemble size in bagging refers to the number of base learners (models) included in the ensemble. The role of ensemble size is crucial in determining the performance and behavior of the bagged ensemble. Here's how the ensemble size affects bagging:\n",
    "\n",
    "1. **Impact on Performance**:\n",
    "   - Increasing the ensemble size generally improves the performance of bagging, up to a certain point. Adding more diverse base learners to the ensemble helps reduce variance and improve generalization performance by capturing different aspects of the data.\n",
    "   - However, after reaching a certain ensemble size, the marginal improvement in performance may diminish, and adding more base learners may lead to diminishing returns or even overfitting if the base learners become too correlated.\n",
    "\n",
    "2. **Tradeoff with Computational Cost**:\n",
    "   - While increasing the ensemble size can improve performance, it also comes with an increase in computational cost. Training and maintaining a larger ensemble require more computational resources and may lead to longer training times and higher memory consumption.\n",
    "   - Therefore, there is a tradeoff between the computational cost and the performance gains achieved by increasing the ensemble size. It's essential to strike a balance between the two factors based on the specific requirements of the problem and available resources.\n",
    "\n",
    "3. **Rule of Thumb**:\n",
    "   - There is no one-size-fits-all answer to how many models should be included in the ensemble. The optimal ensemble size depends on various factors, including the complexity of the problem, the size of the dataset, the diversity of the base learners, and computational constraints.\n",
    "   - As a rule of thumb, ensembles with a moderate number of base learners, such as 50 to 500, often achieve good performance across a wide range of tasks without excessively increasing computational cost.\n",
    "   - However, the optimal ensemble size should be determined empirically through experimentation and cross-validation, considering the specific characteristics of the dataset and the problem domain.\n",
    "\n",
    "4. **Bias-Variance Tradeoff**:\n",
    "   - The ensemble size also influences the bias-variance tradeoff in bagging. A larger ensemble size tends to reduce the variance of the ensemble's predictions by averaging or voting over more diverse base learners. However, it may also increase the bias if the base learners become too similar or if overfitting occurs.\n",
    "\n",
    "In summary, the ensemble size plays a critical role in bagging by balancing the tradeoff between performance improvement and computational cost. While increasing the ensemble size can enhance performance by reducing variance, it's essential to consider computational constraints and avoid overfitting by ensuring diversity among the base learners. Experimentation and cross-validation are key to determining the optimal ensemble size for a given problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96b66d2",
   "metadata": {},
   "source": [
    "### Q6. Can you provide an example of a real-world application of bagging in machine learning?  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6536dc3",
   "metadata": {},
   "source": [
    "Certainly! One real-world application of bagging in machine learning is in the field of finance, particularly in credit scoring for loan approval. Bagging techniques, such as Random Forest, can be employed to improve the accuracy and robustness of credit scoring models. Here's how bagging can be applied in this context:\n",
    "\n",
    "**Real-World Application: Credit Scoring**\n",
    "\n",
    "1. **Problem Description**:\n",
    "   - In the banking and finance industry, lenders evaluate the creditworthiness of loan applicants to determine the risk associated with lending money to them.\n",
    "   - Credit scoring models are used to predict the likelihood of a borrower defaulting on a loan based on various features such as credit history, income, debt-to-income ratio, and employment status.\n",
    "\n",
    "2. **Data Collection**:\n",
    "   - Lenders collect historical data on past loan applicants, including both approved and rejected applications.\n",
    "   - The dataset contains features (e.g., credit score, income, loan amount) and the corresponding labels indicating whether the loan was repaid (positive class) or defaulted (negative class).\n",
    "\n",
    "3. **Model Building**:\n",
    "   - Bagging techniques, such as Random Forest, can be employed to build a credit scoring model.\n",
    "   - Multiple decision trees are trained on different bootstrap samples of the training data, each capturing different patterns and relationships in the data.\n",
    "   - The predictions of individual decision trees are aggregated using majority voting to make the final prediction for each loan application.\n",
    "\n",
    "4. **Advantages of Bagging**:\n",
    "   - Bagging improves the accuracy and robustness of the credit scoring model by reducing variance and overfitting.\n",
    "   - The ensemble of decision trees captures a wide range of features and patterns in the data, making the model more resilient to noise and outliers.\n",
    "   - By aggregating predictions from multiple trees, the model can make more reliable predictions, leading to better risk assessment and decision-making in loan approval.\n",
    "\n",
    "5. **Evaluation and Deployment**:\n",
    "   - The bagged credit scoring model is evaluated using performance metrics such as accuracy, precision, recall, and F1-score on a held-out test dataset.\n",
    "   - Once the model's performance meets the desired criteria, it can be deployed into production to automate the loan approval process.\n",
    "   - Lenders can use the model to assess the credit risk of new loan applicants quickly and accurately, leading to more efficient and consistent decision-making.\n",
    "\n",
    "Overall, bagging techniques like Random Forest can significantly improve the accuracy and reliability of credit scoring models, enabling lenders to make informed decisions about loan approval while managing risk effectively in the financial industry."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b29d45a",
   "metadata": {},
   "source": [
    "# Ensemble Techniques And Its Types-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191aaf23",
   "metadata": {},
   "source": [
    "Q1. What is Random Forest Regressor?  \n",
    "Q2. How does Random Forest Regressor reduce the risk of overfitting?   \n",
    "Q3. How does Random Forest Regressor aggregate the predictions of multiple decision trees?  \n",
    "Q4. What are the hyperparameters of Random Forest Regressor?  \n",
    "Q5. What is the difference between Random Forest Regressor and Decision Tree Regressor?  \n",
    "Q6. What are the advantages and disadvantages of Random Forest Regressor?  \n",
    "Q7. What is the output of Random Forest Regressor?  \n",
    "Q8. Can Random Forest Regressor be used for classification tasks?  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b8469c",
   "metadata": {},
   "source": [
    "### Q1. What is Random Forest Regressor?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccaec7bc",
   "metadata": {},
   "source": [
    "Random Forest Regressor is a type of ensemble learning method used for regression tasks. It belongs to the family of decision tree algorithms and is an extension of the Random Forest Classifier, which is used for classification tasks.\n",
    "\n",
    "In Random Forest Regressor, multiple decision trees are trained on different subsets of the training data using a technique called bagging (Bootstrap Aggregating). Each decision tree is trained independently and makes predictions independently. During the prediction phase, the output of the Random Forest Regressor is the average (for regression tasks) of the predictions made by individual trees, thus reducing overfitting and improving the overall accuracy of the model.\n",
    "\n",
    "Random Forest Regressor is known for its ability to handle high-dimensional datasets with a large number of features, as well as its robustness to noisy data and outliers. It's a popular choice for regression tasks in machine learning due to its simplicity, flexibility, and effectiveness in capturing complex relationships in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58744edd",
   "metadata": {},
   "source": [
    "### Q2. How does Random Forest Regressor reduce the risk of overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacbbf3f",
   "metadata": {},
   "source": [
    "Random Forest Regressor reduces the risk of overfitting through several mechanisms inherent in its design:\n",
    "\n",
    "1. **Ensemble of Trees**: Random Forest Regressor consists of an ensemble of multiple decision trees. Each tree is trained independently on a random subset of the training data and a random subset of features. By averaging the predictions of multiple trees, the model reduces the variance and tends to generalize better to unseen data.\n",
    "\n",
    "2. **Random Feature Selection**: At each split in the decision tree, Random Forest Regressor randomly selects a subset of features from which to choose the best split. This random feature selection helps prevent individual trees from becoming overly specialized to the training data, reducing the likelihood of overfitting.\n",
    "\n",
    "3. **Bootstrap Aggregating (Bagging)**: Random Forest Regressor employs a technique called bagging, which involves training each decision tree on a bootstrapped sample of the training data. This sampling with replacement ensures that each tree sees a slightly different subset of the data, introducing diversity among the trees and reducing overfitting.\n",
    "\n",
    "4. **Pruning**: While decision trees in a Random Forest are allowed to grow without constraints, the averaging effect of combining multiple trees helps mitigate the risk of overfitting. Additionally, individual trees may be pruned during the training process if they exhibit excessive depth or complexity, further improving generalization performance.\n",
    "\n",
    "Overall, by combining the predictions of multiple randomized decision trees and leveraging techniques like random feature selection and bagging, Random Forest Regressor is able to reduce the risk of overfitting and produce more robust models for regression tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7c4090",
   "metadata": {},
   "source": [
    "### Q3. How does Random Forest Regressor aggregate the predictions of multiple decision trees?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2dbc252",
   "metadata": {},
   "source": [
    "Random Forest Regressor aggregates the predictions of multiple decision trees through a process called averaging. Here's how it works:\n",
    "\n",
    "1. **Prediction**: Each decision tree in the Random Forest independently makes a prediction for the target variable based on the input features.\n",
    "\n",
    "2. **Regression**: Since Random Forest is used for regression tasks, each decision tree predicts a continuous numerical value.\n",
    "\n",
    "3. **Averaging**: Once all the decision trees have made their predictions, the Random Forest aggregates these predictions by averaging them. Specifically, the predicted values from all the trees are averaged to produce the final output of the Random Forest Regressor.\n",
    "\n",
    "4. **Weighted Averaging (Optional)**: In some implementations, each tree's prediction may be weighted differently before averaging. The weights can be based on the performance of individual trees or other criteria.\n",
    "\n",
    "5. **Final Prediction**: The aggregated average or weighted average of predictions serves as the final prediction of the Random Forest Regressor for a given input.\n",
    "\n",
    "By aggregating the predictions of multiple trees, Random Forest Regressor reduces the variance of the model and tends to produce more stable and accurate predictions compared to individual decision trees. This averaging process helps in smoothing out the noise and capturing the underlying patterns in the data, leading to better generalization performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31adf022",
   "metadata": {},
   "source": [
    "### Q4. What are the hyperparameters of Random Forest Regressor?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee29d7a",
   "metadata": {},
   "source": [
    "The Random Forest Regressor has several hyperparameters that can be tuned to control the behavior and performance of the model. Some of the key hyperparameters include:\n",
    "\n",
    "1. **n_estimators**: The number of decision trees in the forest. Increasing the number of trees generally improves the performance of the model, but it also increases the computational cost.\n",
    "\n",
    "2. **max_depth**: The maximum depth of each decision tree in the forest. Controlling the maximum depth helps prevent overfitting by limiting the complexity of individual trees.\n",
    "\n",
    "3. **min_samples_split**: The minimum number of samples required to split an internal node during the tree-building process. Increasing this parameter can lead to simpler trees and reduce overfitting.\n",
    "\n",
    "4. **min_samples_leaf**: The minimum number of samples required to be at a leaf node. Similar to min_samples_split, increasing this parameter can regularize the model by preventing the trees from growing too deep.\n",
    "\n",
    "5. **max_features**: The number of features to consider when looking for the best split at each node. This parameter can help control the diversity of trees in the forest.\n",
    "\n",
    "6. **bootstrap**: Whether to bootstrap samples when building trees. Bootstrapping introduces randomness into the training process and can help improve model performance.\n",
    "\n",
    "7. **random_state**: Seed for random number generation. Setting this parameter ensures reproducibility of results.\n",
    "\n",
    "These are some of the most commonly used hyperparameters in Random Forest Regressor. Proper tuning of these hyperparameters can significantly impact the model's performance and generalization ability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02727890",
   "metadata": {},
   "source": [
    "### Q5. What is the difference between Random Forest Regressor and Decision Tree Regressor?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c9ac20",
   "metadata": {},
   "source": [
    "Random Forest Regressor and Decision Tree Regressor are both machine learning algorithms used for regression tasks, but they differ in several key aspects:\n",
    "\n",
    "1. **Algorithm**:\n",
    "   - **Decision Tree Regressor**: It builds a single decision tree by recursively splitting the dataset based on feature values to minimize the variance of the target variable.\n",
    "   - **Random Forest Regressor**: It builds an ensemble of decision trees (a forest) by training multiple decision trees on random subsets of the dataset and averaging their predictions.\n",
    "\n",
    "2. **Bias-Variance Tradeoff**:\n",
    "   - **Decision Tree Regressor**: Decision trees tend to have high variance and are prone to overfitting, especially when they grow deep.\n",
    "   - **Random Forest Regressor**: Random forests mitigate the overfitting problem by aggregating predictions from multiple decision trees, reducing variance and improving generalization performance.\n",
    "\n",
    "3. **Model Complexity**:\n",
    "   - **Decision Tree Regressor**: Decision trees can become very complex, especially if they are allowed to grow deep. This complexity can lead to overfitting.\n",
    "   - **Random Forest Regressor**: Random forests consist of multiple decision trees, typically with limited depth, which helps to control model complexity and prevent overfitting.\n",
    "\n",
    "4. **Training Speed**:\n",
    "   - **Decision Tree Regressor**: Training a single decision tree is usually faster compared to training a random forest because it involves constructing only one tree.\n",
    "   - **Random Forest Regressor**: Training a random forest involves training multiple decision trees, which can be computationally more expensive, especially for large datasets.\n",
    "\n",
    "5. **Prediction Accuracy**:\n",
    "   - **Decision Tree Regressor**: Decision trees may capture complex patterns in the data but are susceptible to overfitting, leading to poor generalization performance.\n",
    "   - **Random Forest Regressor**: Random forests typically have higher prediction accuracy compared to individual decision trees, as they reduce overfitting and improve generalization by combining the predictions of multiple trees.\n",
    "\n",
    "In summary, while Decision Tree Regressor builds a single decision tree, Random Forest Regressor constructs an ensemble of decision trees to achieve better performance and generalization by reducing overfitting.\n",
    "\n",
    "\n",
    "\n",
    "Here's the information presented in a tabular format:\n",
    "\n",
    "| **Feature**                      | **Random Forest Regressor**                                | **Decision Tree Regressor**                                |\n",
    "|---------------------------------|------------------------------------------------------------|------------------------------------------------------------|\n",
    "| Model Type                      | Ensemble model composed of multiple decision trees.        | Single decision tree.                                      |\n",
    "| Overfitting                     | Less prone to overfitting due to ensemble averaging.       | More prone to overfitting, especially with deep trees.     |\n",
    "| Prediction Stability            | More stable predictions due to averaging over trees.       | Less stable predictions, especially with small datasets.   |\n",
    "| Interpretability                | Less interpretable due to ensemble nature.                 | More interpretable as individual trees can be visualized.  |\n",
    "| Feature Importance              | Provides feature importance scores based on ensemble.       | Provides feature importance based on single tree.         |\n",
    "| Computational Complexity        | Higher computational complexity due to multiple trees.     | Lower computational complexity as it's a single tree.      |\n",
    "| Handling Outliers and Noise     | Robust to outliers and noise due to ensemble averaging.    | Sensitive to outliers and noise, especially with deep trees.|\n",
    "| Memory Usage                    | Requires more memory to store multiple trees.              | Requires less memory as it's a single tree.               |\n",
    "| Training Time                   | Longer training time due to multiple trees.                | Shorter training time compared to random forests.         |\n",
    "| Performance in Small Datasets   | Generally performs well even with small datasets.          | May overfit with small datasets due to high variance.     |\n",
    "| Performance in Large Datasets   | Suitable for large datasets due to parallelization.        | May struggle with large datasets due to computational complexity. |\n",
    "\n",
    "This table summarizes the key differences between Random Forest Regressor and Decision Tree Regressor in various aspects such as model type, overfitting, interpretability, computational complexity, and performance in different dataset sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae004e23",
   "metadata": {},
   "source": [
    "### Q6. What are the advantages and disadvantages of Random Forest Regressor?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efec095",
   "metadata": {},
   "source": [
    "Random Forest Regressor has several advantages and disadvantages:\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "1. **High Accuracy:** Random forests generally have high prediction accuracy due to their ability to capture complex relationships in the data by combining multiple decision trees.\n",
    "   \n",
    "2. **Reduced Overfitting:** By aggregating predictions from multiple decision trees, random forests mitigate the overfitting problem typically associated with individual decision trees.\n",
    "   \n",
    "3. **Robustness to Outliers:** Random forests are robust to outliers and noise in the data because they consider multiple trees, which helps in making more robust predictions.\n",
    "   \n",
    "4. **Implicit Feature Selection:** Random forests perform implicit feature selection by measuring the importance of features based on their contribution to reducing impurity in decision trees.\n",
    "\n",
    "5. **Handling Missing Values:** Random forests can handle missing values in the dataset without requiring imputation, as they make predictions based on available features in each tree.\n",
    "\n",
    "6. **Parallelization:** Training and prediction in random forests can be parallelized, making them suitable for large datasets and distributed computing environments.\n",
    "\n",
    "**Disadvantages:**\n",
    "\n",
    "1. **Computational Complexity:** Building a random forest involves training multiple decision trees, which can be computationally expensive, especially for large datasets with many features and trees.\n",
    "   \n",
    "2. **Less Interpretability:** While decision trees are interpretable, random forests are less interpretable due to the ensemble nature of the model, making it challenging to understand the underlying decision-making process.\n",
    "   \n",
    "3. **Memory Usage:** Random forests require storing multiple decision trees in memory, which can consume a significant amount of memory, especially for large forests with deep trees.\n",
    "   \n",
    "4. **Hyperparameter Tuning:** Random forests have several hyperparameters that need to be tuned for optimal performance, such as the number of trees, maximum depth of trees, and minimum number of samples required to split a node.\n",
    "   \n",
    "5. **Bias Towards Majority Class:** Random forests can exhibit a bias towards the majority class in imbalanced datasets, leading to suboptimal performance for minority classes unless appropriate class weighting or sampling techniques are applied.\n",
    "\n",
    "Despite these disadvantages, random forests are widely used in practice and often yield excellent results across various regression tasks, especially when tuned properly and used in combination with other ensemble methods.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a49ffcd",
   "metadata": {},
   "source": [
    "### Q7. What is the output of Random Forest Regressor?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e919e8",
   "metadata": {},
   "source": [
    "The output of a Random Forest Regressor depends on whether it's used for regression or classification tasks.\n",
    "\n",
    "For a regression task:\n",
    "- The output of a Random Forest Regressor is a continuous numerical value.\n",
    "- Each decision tree in the forest predicts a numerical value for the input data.\n",
    "- The final output of the random forest is typically the average (or sometimes the median) of the predictions made by all the individual trees.\n",
    "\n",
    "For a classification task:\n",
    "- The output of a Random Forest Regressor is a class label or a probability score.\n",
    "- Each decision tree in the forest predicts a class label or probability for the input data.\n",
    "- The final output of the random forest is determined by majority voting (for classification) or averaging probabilities (for probability estimation) from all the individual trees.\n",
    "\n",
    "In both cases, the output is determined based on the ensemble predictions of multiple decision trees, which are aggregated to provide a more robust and accurate prediction compared to a single decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda83c75",
   "metadata": {},
   "source": [
    "### Q8. Can Random Forest Regressor be used for classification tasks?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0455b8d1",
   "metadata": {},
   "source": [
    "The Random Forest algorithm is primarily designed for classification tasks, although it can also be adapted for regression tasks. \n",
    "\n",
    "- For classification tasks, Random Forest builds a collection of decision trees and outputs the class that is the mode of the classes (classification) or the class probabilities (probability estimation) of the individual trees.\n",
    "  \n",
    "- For regression tasks, Random Forest computes the average (or sometimes median) prediction of the individual trees in the forest.\n",
    "\n",
    "So, while Random Forest is commonly used for classification tasks, it can technically be used for regression as well. However, it's essential to understand the appropriate use case and interpret the output accordingly based on whether it's used for classification or regression.\n",
    "\n",
    "For classification tasks, Random Forest employs a voting mechanism where each tree in the forest independently predicts a class, and the final prediction is determined by the majority class among all the trees.\n",
    "\n",
    "For regression tasks, Random Forest computes the average (or sometimes median) prediction of the individual trees in the forest. This aggregated prediction provides an estimation of the target variable for regression problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6142054",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a32841f0",
   "metadata": {},
   "source": [
    "# Ensemble Techniques And Its Types-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e631f71",
   "metadata": {},
   "source": [
    "Build a random forest classifier to predict the risk of heart disease based on a dataset of patient\n",
    "information. The dataset contains 303 instances with 14 features, including age, sex, chest pain type,\n",
    "resting blood pressure, serum cholesterol, and maximum heart rate achieved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4e2d6d",
   "metadata": {},
   "source": [
    "Q1. Preprocess the dataset by handling missing values, encoding categorical variables, and scaling the\n",
    "numerical features if necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521d7d54",
   "metadata": {},
   "source": [
    "Q2. Split the dataset into a training set (70%) and a test set (30%)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c992a417",
   "metadata": {},
   "source": [
    "Q3. Train a random forest classifier on the training set using 100 trees and a maximum depth of 10 for each\n",
    "tree. Use the default values for other hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8c264f",
   "metadata": {},
   "source": [
    "Q4. Evaluate the performance of the model on the test set using accuracy, precision, recall, and F1 score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5192414f",
   "metadata": {},
   "source": [
    "Q5. Use the feature importance scores to identify the top 5 most important features in predicting heart\n",
    "disease risk. Visualise the feature importances using a bar chart."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f0ce6a",
   "metadata": {},
   "source": [
    "Q6. Tune the hyperparameters of the random forest classifier using grid search or random search. Try\n",
    "different values of the number of trees, maximum depth, minimum samples split, and minimum samples\n",
    "leaf. Use 5-fold cross-validation to evaluate the performance of each set of hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9506c974",
   "metadata": {},
   "source": [
    "Q7. Report the best set of hyperparameters found by the search and the corresponding performance\n",
    "metrics. Compare the performance of the tuned model with the default model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e14ecec",
   "metadata": {},
   "source": [
    "Q8. Interpret the model by analysing the decision boundaries of the random forest classifier. Plot the\n",
    "decision boundaries on a scatter plot of two of the most important features. Discuss the insights and\n",
    "limitations of the model for predicting heart disease risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f36ab7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import requests\n",
    "from io import StringIO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44158100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset\n",
    "url = \"https://drive.google.com/uc?id=1bGoIE4Z2kG5nyh-fGZAJ7LH0ki3UfmSJ\"\n",
    "download = requests.get(url).content\n",
    "df = pd.read_csv(StringIO(download.decode('utf-8')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c51bb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "# Handle missing values\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28caa1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical variables\n",
    "df = pd.get_dummies(df, columns=['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'thal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de47f4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train and test sets\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83ad9327",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(max_depth=10)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(max_depth=10)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(max_depth=10)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a random forest classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, max_depth=10)\n",
    "rf_classifier.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73458fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the performance\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88d81e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8131868131868132\n",
      "Precision: 0.8367346938775511\n",
      "Recall: 0.82\n",
      "F1 Score: 0.8282828282828283\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46c02e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "feature_importances = rf_classifier.feature_importances_\n",
    "sorted_indices = np.argsort(feature_importances)[::-1]\n",
    "top_features = X.columns[sorted_indices][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6625fa1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(top_features)), feature_importances[sorted_indices][:5], align='center')\n",
    "plt.xticks(range(len(top_features)), top_features, rotation=45)\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Importance Score')\n",
    "plt.title('Top 5 Feature Importances')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8542949a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5, estimator=RandomForestClassifier(), n_jobs=-1,\n",
       "             param_grid={&#x27;max_depth&#x27;: [5, 10, 15],\n",
       "                         &#x27;min_samples_leaf&#x27;: [1, 2, 4],\n",
       "                         &#x27;min_samples_split&#x27;: [2, 5, 10],\n",
       "                         &#x27;n_estimators&#x27;: [100, 200, 300]})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5, estimator=RandomForestClassifier(), n_jobs=-1,\n",
       "             param_grid={&#x27;max_depth&#x27;: [5, 10, 15],\n",
       "                         &#x27;min_samples_leaf&#x27;: [1, 2, 4],\n",
       "                         &#x27;min_samples_split&#x27;: [2, 5, 10],\n",
       "                         &#x27;n_estimators&#x27;: [100, 200, 300]})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5, estimator=RandomForestClassifier(), n_jobs=-1,\n",
       "             param_grid={'max_depth': [5, 10, 15],\n",
       "                         'min_samples_leaf': [1, 2, 4],\n",
       "                         'min_samples_split': [2, 5, 10],\n",
       "                         'n_estimators': [100, 200, 300]})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q6. Hyperparameter tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [5, 10, 15],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "grid_search = GridSearchCV(estimator=RandomForestClassifier(), param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b1b507d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7. Report best hyperparameters and performance\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred_tuned = best_model.predict(X_test)\n",
    "accuracy_tuned = accuracy_score(y_test, y_pred_tuned)\n",
    "precision_tuned = precision_score(y_test, y_pred_tuned)\n",
    "recall_tuned = recall_score(y_test, y_pred_tuned)\n",
    "f1_tuned = f1_score(y_test, y_pred_tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c29a0ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'max_depth': 10, 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 300}\n",
      "Tuned Model Accuracy: 0.8461538461538461\n",
      "Tuned Model Precision: 0.86\n",
      "Tuned Model Recall: 0.86\n",
      "Tuned Model F1 Score: 0.8599999999999999\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Tuned Model Accuracy:\", accuracy_tuned)\n",
    "print(\"Tuned Model Precision:\", precision_tuned)\n",
    "print(\"Tuned Model Recall:\", recall_tuned)\n",
    "print(\"Tuned Model F1 Score:\", f1_tuned)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb738305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default Model Accuracy: 0.8131868131868132\n",
      "Default Model Precision: 0.8367346938775511\n",
      "Default Model Recall: 0.82\n",
      "Default Model F1 Score: 0.8282828282828283\n"
     ]
    }
   ],
   "source": [
    "# Compare with default model\n",
    "print(\"Default Model Accuracy:\", accuracy)\n",
    "print(\"Default Model Precision:\", precision)\n",
    "print(\"Default Model Recall:\", recall)\n",
    "print(\"Default Model F1 Score:\", f1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c53c37",
   "metadata": {},
   "source": [
    "# Ensemble Techniques And Its Types-5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b346b7b8",
   "metadata": {},
   "source": [
    "Q1. You are working on a machine learning project where you have a dataset containing numerical and categorical features. You have identified that some of the features are highly correlated and there are missing values in some of the columns. You want to build a pipeline that automates the feature engineering process and handles the missing values.\n",
    "\n",
    "Design a pipeline that includes the following steps:\n",
    "\n",
    "- Use an automated feature selection method to identify the important features in the dataset.\n",
    "- Create a numerical pipeline that includes the following steps:\n",
    "  - Impute the missing values in the numerical columns using the mean of the column values.\n",
    "  - Scale the numerical columns using standardization.\n",
    "- Create a categorical pipeline that includes the following steps:\n",
    "  - Impute the missing values in the categorical columns using the most frequent value of the column.\n",
    "  - One-hot encode the categorical columns.\n",
    "- Combine the numerical and categorical pipelines using a ColumnTransformer.\n",
    "- Use a Random Forest Classifier to build the final model.\n",
    "- Evaluate the accuracy of the model on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "31217afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7692307692307693\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "url = \"https://drive.google.com/uc?id=1bGoIE4Z2kG5nyh-fGZAJ7LH0ki3UfmSJ\"\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "# Separate features and target variable\n",
    "X = data.drop(columns=['target'])\n",
    "y = data['target']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Define numerical pipeline\n",
    "numerical_features = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\n",
    "numerical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Define categorical pipeline (assuming 'sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal' are categorical)\n",
    "categorical_features = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal']\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine numerical and categorical pipelines\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numerical_pipeline, numerical_features),\n",
    "    ('cat', categorical_pipeline, categorical_features)\n",
    "])\n",
    "\n",
    "# Define the final pipeline with feature selection and Random Forest classifier\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('feature_selection', SelectFromModel(RandomForestClassifier())),\n",
    "    ('classifier', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800eeeaa",
   "metadata": {},
   "source": [
    "Q2. Build a pipeline that includes a Random Forest Classifier and a Logistic Regression Classifier, and then use a Voting Classifier to combine their predictions. Train the pipeline on the dataset and evaluate its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e381330a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8021978021978022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\inare\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "# Assuming the dataset is loaded into a DataFrame called 'data'\n",
    "\n",
    "# Split the data into features and target\n",
    "X = data.drop(columns=['target'])\n",
    "y = data['target']\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Build pipelines for Random Forest and Logistic Regression classifiers\n",
    "rf_pipeline = Pipeline([\n",
    "    ('rf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "lr_pipeline = Pipeline([\n",
    "    ('lr', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Combine the pipelines into a Voting Classifier\n",
    "voting_pipeline = VotingClassifier(estimators=[\n",
    "    ('rf', rf_pipeline),\n",
    "    ('lr', lr_pipeline)\n",
    "], voting='hard')\n",
    "\n",
    "# Train the voting classifier\n",
    "voting_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate accuracy on the test set\n",
    "accuracy = accuracy_score(y_test, voting_pipeline.predict(X_test))\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e5e34b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
