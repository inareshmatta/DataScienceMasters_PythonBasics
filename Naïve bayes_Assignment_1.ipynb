{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "218dda2b",
   "metadata": {},
   "source": [
    "Q1. What is Bayes' theorem?  \n",
    "Q2. What is the formula for Bayes' theorem?  \n",
    "Q3. How is Bayes' theorem used in practice?  \n",
    "Q4. What is the relationship between Bayes' theorem and conditional probability?  \n",
    "Q5. How do you choose which type of Naive Bayes classifier to use for any given problem?  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77ba119",
   "metadata": {},
   "source": [
    "Q6. Assignment:   \n",
    "You have a dataset with two features, X1 and X2, and two possible classes, A and B. You want to use Naive  \n",
    "Bayes to classify a new instance with features X1 = 3 and X2 = 4. The following table shows the frequency of  \n",
    "each feature value for each class:  \n",
    "\n",
    "\n",
    "Class X1=1 X1=2 X1=3 X2=1 X2=2 X2=3 X2=4  \n",
    "A 3 3 4 4 3 3 3  \n",
    "B 2 2 1 2 2 2 3  \n",
    "\n",
    "\n",
    "Assuming equal prior probabilities for each class, which class would Naive Bayes predict the new instance\n",
    "to belong to?  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb5b206",
   "metadata": {},
   "source": [
    "### Q1. What is Bayes' theorem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10969519",
   "metadata": {},
   "source": [
    "Bayes' theorem is a fundamental concept in probability theory and statistics named after the Reverend Thomas Bayes. It provides a way to update our beliefs about the probability of an event occurring based on new evidence or information. The theorem is expressed mathematically as follows:\n",
    "\n",
    "$ P(A|B) = \\frac{P(B|A) \\times P(A)}{P(B)} $\n",
    "\n",
    "Where:\n",
    "- \\( P(A|B) \\) is the posterior probability of event A given evidence B.\n",
    "- \\( P(B|A) \\) is the probability of observing evidence B given that A is true, often referred to as the likelihood.\n",
    "- \\( P(A) \\) is the prior probability of event A before considering evidence B.\n",
    "- \\( P(B) \\) is the probability of observing evidence B.\n",
    "\n",
    "In words, Bayes' theorem states that the probability of A given B is proportional to the probability of B given A, multiplied by the prior probability of A, and then normalized by the probability of B.\n",
    "\n",
    "Bayes' theorem has wide-ranging applications in various fields, including statistics, machine learning, data science, and artificial intelligence. It's commonly used in Bayesian inference, where it helps update prior beliefs based on observed data, and in Bayesian statistics for parameter estimation and hypothesis testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb29a4e8",
   "metadata": {},
   "source": [
    "### Q2. What is the formula for Bayes' theorem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7307c68e",
   "metadata": {},
   "source": [
    "The formula for Bayes' theorem is as follows:\n",
    "\n",
    "$ P(A|B) = \\frac{P(B|A) \\times P(A)}{P(B)} $\n",
    "\n",
    "Where:\n",
    "- \\( P(A|B) \\) is the posterior probability of event A given evidence B.\n",
    "- \\( P(B|A) \\) is the probability of observing evidence B given that A is true, often referred to as the likelihood.\n",
    "- \\( P(A) \\) is the prior probability of event A before considering evidence B.\n",
    "- \\( P(B) \\) is the probability of observing evidence B.\n",
    "\n",
    "This formula provides a way to update our beliefs about the probability of an event occurring based on new evidence or information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fa7d80",
   "metadata": {},
   "source": [
    "### Q3. How is Bayes' theorem used in practice?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f84f30d",
   "metadata": {},
   "source": [
    "Bayes' theorem is used in practice in various fields, including but not limited to:\n",
    "\n",
    "1. **Medical Diagnosis**: Bayes' theorem is employed in medical diagnosis to update the probability of a patient having a particular disease given their symptoms, based on the likelihood of those symptoms occurring if the patient has the disease and the prior probability of the disease in the population.\n",
    "\n",
    "2. **Spam Filtering**: In email spam filtering, Bayes' theorem is utilized to determine the probability that an email is spam given certain words or patterns in the email content. The prior probability is based on the overall prevalence of spam emails, and the likelihood is based on the probability of certain words appearing in spam emails versus legitimate ones.\n",
    "\n",
    "3. **Machine Learning and Pattern Recognition**: Bayes' theorem serves as the foundation for Bayesian machine learning algorithms, such as Naive Bayes classifiers. These algorithms use probabilistic models to classify data points based on observed features, updating their beliefs about class membership as new data becomes available.\n",
    "\n",
    "4. **Risk Assessment and Decision Making**: Bayes' theorem is applied in risk assessment and decision making processes, such as in finance, insurance, and engineering. It allows decision-makers to update their estimates of the likelihood of different outcomes based on new information or data.\n",
    "\n",
    "5. **Natural Language Processing**: In natural language processing tasks like language modeling and text classification, Bayes' theorem is used to estimate the likelihood of observing certain sequences of words or phrases given a particular context, enabling tasks such as language generation, sentiment analysis, and document categorization.\n",
    "\n",
    "6. **Fault Diagnosis in Engineering**: Bayes' theorem is utilized in fault diagnosis systems to assess the probability of different components or systems being faulty given observed symptoms or sensor readings, enabling predictive maintenance and troubleshooting in complex systems.\n",
    "\n",
    "In all these applications, Bayes' theorem provides a principled framework for updating beliefs or probabilities based on new evidence, making it a powerful tool for reasoning under uncertainty in various domains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda9c2ac",
   "metadata": {},
   "source": [
    "### Q4. What is the relationship between Bayes' theorem and conditional probability?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d5966e",
   "metadata": {},
   "source": [
    "Bayes' theorem is closely related to conditional probability. In fact, Bayes' theorem is derived from the definition of conditional probability. \n",
    "\n",
    "Conditional probability is the probability of an event occurring given that another event has already occurred. It is denoted as \\( P(A|B) \\), read as \"the probability of event A given event B.\" \n",
    "\n",
    "Bayes' theorem provides a way to compute conditional probabilities in cases where it might not be straightforward to do so directly. It states that the probability of event A given event B (\\( P(A|B) \\)) is equal to the probability of event B given event A (\\( P(B|A) \\)) times the probability of event A (\\( P(A) \\)), divided by the probability of event B (\\( P(B) \\)). \n",
    "\n",
    "So, Bayes' theorem mathematically relates conditional probabilities as follows:\n",
    "\n",
    "$ P(A|B) = \\frac{P(B|A) \\times P(A)}{P(B)} $\n",
    "\n",
    "In summary, while conditional probability focuses on the likelihood of an event given another event has occurred, Bayes' theorem provides a framework for updating these conditional probabilities based on new evidence or information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d70a98",
   "metadata": {},
   "source": [
    "### Q5. How do you choose which type of Naive Bayes classifier to use for any given problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71365813",
   "metadata": {},
   "source": [
    "Choosing the appropriate type of Naive Bayes classifier for a given problem depends on several factors, including the nature of the problem, the characteristics of the data, and the assumptions you're willing to make. Here's a brief overview of the different types of Naive Bayes classifiers and considerations for choosing between them:\n",
    "\n",
    "1. **Gaussian Naive Bayes**:\n",
    "   - This classifier assumes that features follow a Gaussian (normal) distribution.\n",
    "   - It is suitable for continuous features.\n",
    "   - If your features are continuous and can be reasonably assumed to be normally distributed, Gaussian Naive Bayes may be a good choice.\n",
    "\n",
    "2. **Multinomial Naive Bayes**:\n",
    "   - This classifier is appropriate for features that represent counts or frequencies.\n",
    "   - It is commonly used in text classification problems, where features represent word counts or term frequencies.\n",
    "   - If your features are categorical or represent counts of occurrences, Multinomial Naive Bayes is often a good choice.\n",
    "\n",
    "3. **Bernoulli Naive Bayes**:\n",
    "   - This classifier is similar to Multinomial Naive Bayes but assumes binary features (i.e., presence or absence of a feature).\n",
    "   - It is commonly used for binary feature vectors, such as presence/absence of words in text classification.\n",
    "   - If your features are binary or can be represented as binary (e.g., presence/absence of certain attributes), Bernoulli Naive Bayes is a suitable choice.\n",
    "\n",
    "When choosing the type of Naive Bayes classifier, consider the following:\n",
    "\n",
    "- **Nature of Features**: Determine whether your features are continuous, categorical, or binary.\n",
    "- **Distribution of Features**: Assess whether the assumption of feature distribution made by each type of Naive Bayes classifier (Gaussian, Multinomial, Bernoulli) aligns with the actual distribution of your data.\n",
    "- **Size of the Dataset**: Consider the size of your dataset and the computational complexity of each classifier. Some classifiers may perform better than others with small or large datasets.\n",
    "- **Performance Metrics**: Evaluate the performance of each classifier using appropriate evaluation metrics (e.g., accuracy, precision, recall, F1-score) on a validation dataset or through cross-validation.\n",
    "\n",
    "Ultimately, the choice of Naive Bayes classifier should be guided by empirical evaluation and domain knowledge about the problem at hand. It's often a good idea to experiment with different classifiers and select the one that performs best for your specific task and dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9caa4a82",
   "metadata": {},
   "source": [
    "### Q6. Assignment: You have a dataset with two features, X1 and X2, and two possible classes, A and B. You want to use Naive\n",
    "### Bayes to classify a new instance with features X1 = 3 and X2 = 4. The following table shows the frequency of\n",
    "### each feature value for each class:\n",
    "\n",
    "Class X1=1 X1=2 X1=3 X2=1 X2=2 X2=3 X2=4  \n",
    "A 3 3 4 4 3 3 3  \n",
    "B 2 2 1 2 2 2 3  \n",
    "\n",
    "### Assuming equal prior probabilities for each class, which class would Naive Bayes predict the new instance to belong to?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e57ff8",
   "metadata": {},
   "source": [
    "To classify the new instance with features \\(X_1 = 3\\) and \\(X_2 = 4\\) using Naive Bayes, we need to calculate the posterior probabilities for each class given the feature values. Since the prior probabilities for each class are equal, we can ignore them in our calculations.\n",
    "\n",
    "We'll use the Multinomial Naive Bayes classifier because the features are categorical (discrete counts).\n",
    "\n",
    "The probability of each class given the feature values can be calculated using Bayes' theorem as follows:\n",
    "\n",
    "$ P(A|\\text{features}) = \\frac{P(\\text{features}|A) \\times P(A)}{P(\\text{features})} $\n",
    "$ P(B|\\text{features}) = \\frac{P(\\text{features}|B) \\times P(B)}{P(\\text{features})} $\n",
    "\n",
    "Since the prior probabilities are equal, we can ignore them. Therefore, we need to compute \\( P(\\text{features}|A) \\) and \\( P(\\text{features}|B) \\).\n",
    "\n",
    "Let's calculate these probabilities using the given frequency table:\n",
    "\n",
    "For Class A:\n",
    "- $( P(X_1=3|\\text{A}) = \\frac{4}{13} )$\n",
    "- $( P(X_2=4|\\text{A}) = \\frac{3}{13} )$\n",
    "\n",
    "For Class B:\n",
    "- $( P(X_1=3|\\text{B}) = \\frac{1}{9} )$\n",
    "- $( P(X_2=4|\\text{B}) = \\frac{3}{9} )$\n",
    "\n",
    "Now, let's multiply these probabilities together to get the likelihood of each class given the feature values:\n",
    "\n",
    "For Class A:\n",
    "- $( P(\\text{features}|A) = \\frac{4}{13} \\times \\frac{3}{13} = \\frac{12}{169} )$\n",
    "\n",
    "For Class B:\n",
    "- $( P(\\text{features}|B) = \\frac{1}{9} \\times \\frac{3}{9} = \\frac{3}{81} )$\n",
    "\n",
    "Finally, we need to normalize these probabilities:\n",
    "\n",
    "$ P(A|\\text{features}) = \\frac{\\frac{12}{169}}{\\frac{12}{169} + \\frac{3}{81}} $\n",
    "$ P(B|\\text{features}) = \\frac{\\frac{3}{81}}{\\frac{12}{169} + \\frac{3}{81}} $\n",
    "\n",
    "Let's compute these probabilities:\n",
    "\n",
    "For Class A:\n",
    "$ P(A|\\text{features}) = \\frac{\\frac{12}{169}}{\\frac{12}{169} + \\frac{3}{81}} \\approx 0.805 $\n",
    "\n",
    "For Class B:\n",
    "$ P(B|\\text{features}) = \\frac{\\frac{3}{81}}{\\frac{12}{169} + \\frac{3}{81}} \\approx 0.195 $\n",
    "\n",
    "Since $(P(A|\\text{features}) > P(B|\\text{features}))$ , Naive Bayes would predict the new instance to belong to Class A."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62614721",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
