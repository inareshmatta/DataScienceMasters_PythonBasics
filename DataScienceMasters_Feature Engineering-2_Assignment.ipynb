{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31a3166c",
   "metadata": {},
   "source": [
    "### Q1. What is the Filter method in feature selection, and how does it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2ef570",
   "metadata": {},
   "source": [
    "**Filter Method in Feature Selection:**\n",
    "- The filter method in feature selection is a technique that assesses the relevance of features based on certain statistical measures or scoring criteria. It evaluates each feature independently of the machine learning model.\n",
    "\n",
    "**How it Works:**\n",
    "1. **Scoring Criteria:**\n",
    "   - The filter method assigns a score to each feature based on a predefined metric or statistical test.\n",
    "   - Common metrics include correlation, mutual information, chi-square, information gain, and others.\n",
    "   - The goal is to capture the relationship or information content of each feature with respect to the target variable.\n",
    "\n",
    "2. **Ranking Features:**\n",
    "   - Features are ranked or scored based on their individual performance according to the chosen criteria.\n",
    "   - Higher scores indicate stronger relationships or higher information content.\n",
    "\n",
    "3. **Selection Threshold:**\n",
    "   - A threshold is set to determine which features to select or retain.\n",
    "   - Features above the threshold are considered relevant and are retained for further analysis.\n",
    "\n",
    "4. **Independence of Features:**\n",
    "   - The filter method treats each feature independently, meaning it does not consider interactions or dependencies between features.\n",
    "   - Features are selected or rejected based solely on their individual characteristics.\n",
    "\n",
    "5. **Pre-Modeling Stage:**\n",
    "   - The filter method is applied before the model training stage.\n",
    "   - It helps reduce the dimensionality of the dataset by selecting a subset of the most informative features.\n",
    "\n",
    "6. **Computational Efficiency:**\n",
    "   - Filter methods are computationally efficient because they do not involve training a machine learning model.\n",
    "   - Feature selection is based on simple statistical calculations or information theory.\n",
    "\n",
    "7. **Example:**\n",
    "   - For a classification task, the filter method might use the chi-square statistic to measure the dependence between each categorical feature and the target class.\n",
    "   - Features with high chi-square values (indicating significant association with the target class) are retained.\n",
    "\n",
    "**Advantages:**\n",
    "- **Speed:** Filter methods are fast and scalable to large datasets.\n",
    "- **Independence:** They don't rely on the choice of a specific machine learning model.\n",
    "- **Interpretability:** The selected features often have straightforward interpretations based on the chosen metric.\n",
    "\n",
    "**Considerations:**\n",
    "- While efficient, filter methods may overlook feature interactions that are crucial for certain models.\n",
    "- The choice of the scoring metric depends on the nature of the data and the task at hand.\n",
    "- It is a good preprocessing step to reduce dimensionality but might not capture complex relationships in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fae892",
   "metadata": {},
   "source": [
    "### Q2. How does the Wrapper method differ from the Filter method in feature selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad07219",
   "metadata": {},
   "source": [
    "**Wrapper Method in Feature Selection:**\n",
    "- The wrapper method in feature selection evaluates subsets of features by training a machine learning model using different combinations. It assesses the performance of the model with each subset.\n",
    "\n",
    "**Differences between Wrapper and Filter Methods:**\n",
    "\n",
    "1. **Evaluation Criteria:**\n",
    "   - **Filter Method:** Uses predefined statistical metrics or scoring criteria to evaluate the relevance of individual features.\n",
    "   - **Wrapper Method:** Involves training a machine learning model and assessing performance based on model-specific evaluation criteria (e.g., accuracy, F1 score).\n",
    "\n",
    "2. **Interaction with the Model:**\n",
    "   - **Filter Method:** Features are evaluated independently of the machine learning model. No model training is involved in the feature selection process.\n",
    "   - **Wrapper Method:** Features are selected or rejected based on their impact on the performance of a specific machine learning model. It considers feature interactions.\n",
    "\n",
    "3. **Search Strategy:**\n",
    "   - **Filter Method:** Employs a global approach, considering each feature independently and selecting or rejecting features based on a predefined threshold.\n",
    "   - **Wrapper Method:** Employs a local search strategy, exploring different subsets of features and evaluating their performance in the context of the chosen machine learning model.\n",
    "\n",
    "4. **Computational Intensity:**\n",
    "   - **Filter Method:** Generally computationally efficient since it doesn't involve training a model.\n",
    "   - **Wrapper Method:** More computationally intensive as it requires training the machine learning model multiple times for different feature subsets.\n",
    "\n",
    "5. **Model Dependency:**\n",
    "   - **Filter Method:** Model-independent; can be applied to any dataset without regard to the specific machine learning algorithm used.\n",
    "   - **Wrapper Method:** Model-dependent; the choice of the machine learning model used in the wrapper method affects the feature selection process.\n",
    "\n",
    "6. **Example:**\n",
    "   - **Filter Method:** Selecting features with high correlation using a correlation coefficient threshold.\n",
    "   - **Wrapper Method:** Using forward selection or backward elimination with a decision tree classifier, evaluating subsets of features based on classification accuracy.\n",
    "\n",
    "**Advantages of Wrapper Method:**\n",
    "- **Model-Specific:** Identifies features that improve the performance of a specific model.\n",
    "- **Considers Interactions:** Takes into account feature interactions, which can be important for certain models.\n",
    "\n",
    "**Considerations:**\n",
    "- **Computational Cost:** Wrapper methods can be computationally expensive, especially for large datasets or complex models.\n",
    "- **Overfitting Risk:** There is a risk of overfitting to the specific model used in the wrapper method.\n",
    "- **Model Selection:** The choice of the machine learning model in the wrapper method affects the feature selection outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5065e898",
   "metadata": {},
   "source": [
    "### Q3. What are some common techniques used in Embedded feature selection methods?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e59207",
   "metadata": {},
   "source": [
    "**Embedded Feature Selection:**\n",
    "- Embedded feature selection methods incorporate feature selection as an integral part of the model training process. Features are selected or weighted during the model training, and the model learns to give importance to relevant features.\n",
    "\n",
    "**Common Techniques in Embedded Feature Selection:**\n",
    "\n",
    "1. **LASSO (Least Absolute Shrinkage and Selection Operator):**\n",
    "   - **Description:** Adds a penalty term to the linear regression objective function, promoting sparsity in the coefficients and automatically selecting relevant features.\n",
    "   - **Application:** Linear regression, logistic regression.\n",
    "\n",
    "2. **Ridge Regression:**\n",
    "   - **Description:** Similar to LASSO but uses a different penalty term (L2 regularization) to prevent large coefficients. It tends to shrink coefficients toward zero but does not result in sparsity.\n",
    "   - **Application:** Linear regression, logistic regression.\n",
    "\n",
    "3. **Elastic Net:**\n",
    "   - **Description:** Combines L1 and L2 regularization terms, providing a balance between sparsity and coefficient shrinkage.\n",
    "   - **Application:** Linear regression, logistic regression.\n",
    "\n",
    "4. **Decision Trees with Feature Importance:**\n",
    "   - **Description:** Decision trees can assign importance scores to features based on their contribution to the model's performance. Random Forest and Gradient Boosted Trees are commonly used.\n",
    "   - **Application:** Classification, regression.\n",
    "\n",
    "5. **Recursive Feature Elimination (RFE):**\n",
    "   - **Description:** A backward elimination technique that recursively removes the least important features based on the model's coefficients or feature importance scores.\n",
    "   - **Application:** Linear models, support vector machines.\n",
    "\n",
    "6. **Regularized Linear Models (e.g., Regularized Regression):**\n",
    "   - **Description:** Linear models with regularization terms (L1 or L2) to control the complexity of the model and automatically select features.\n",
    "   - **Application:** Linear regression, logistic regression.\n",
    "\n",
    "7. **XGBoost Feature Importance:**\n",
    "   - **Description:** XGBoost, a gradient boosting algorithm, provides a feature importance metric based on the contribution of each feature to the model's predictive performance.\n",
    "   - **Application:** Classification, regression.\n",
    "\n",
    "8. **L1-Regularized SVM (Support Vector Machine):**\n",
    "   - **Description:** Applies L1 regularization to SVM, promoting sparsity in the support vector coefficients and automatically selecting relevant features.\n",
    "   - **Application:** Classification, regression.\n",
    "\n",
    "9. **Neural Networks with Dropout:**\n",
    "   - **Description:** Dropout is a regularization technique applied to neural networks, randomly dropping out neurons during training, which can lead to implicit feature selection.\n",
    "   - **Application:** Deep learning.\n",
    "\n",
    "10. **GLM (Generalized Linear Models) with Regularization:**\n",
    "    - **Description:** Generalized linear models with regularization terms to prevent overfitting and select relevant features.\n",
    "    - **Application:** Regression, classification.\n",
    "\n",
    "**Advantages of Embedded Feature Selection:**\n",
    "- **Model Integration:** Features are selected during the model training process, optimizing the model for predictive performance.\n",
    "- **Automated Selection:** The model learns to assign weights or importance scores to features, automating the selection process.\n",
    "\n",
    "**Considerations:**\n",
    "- **Model-Specific:** The effectiveness of embedded methods may depend on the choice of the underlying model.\n",
    "- **Computational Cost:** Some embedded methods can be computationally intensive, especially for complex models.\n",
    "- **Interpretability:** Interpretability of feature importance scores may vary depending on the model used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a7432a",
   "metadata": {},
   "source": [
    "### Q4. What are some drawbacks of using the Filter method for feature selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bb5aa7",
   "metadata": {},
   "source": [
    "**Drawbacks of the Filter Method:**\n",
    "\n",
    "1. **Independence of Features:**\n",
    "   - The filter method evaluates features independently, ignoring potential interactions or dependencies between features. This can lead to the selection of features that might be important only in combination with others.\n",
    "\n",
    "2. **Lack of Model Context:**\n",
    "   - Filter methods do not consider the context of a specific machine learning model. The selected features may not be the most relevant for the chosen model, leading to suboptimal performance.\n",
    "\n",
    "3. **Limited to Univariate Analysis:**\n",
    "   - Many filter methods rely on univariate analysis, considering the relationship between each feature and the target variable in isolation. This approach may not capture complex patterns that involve multiple features.\n",
    "\n",
    "4. **Threshold Sensitivity:**\n",
    "   - The effectiveness of the filter method depends on setting an appropriate threshold for feature selection. Choosing an arbitrary threshold can result in either too many or too few features being selected, impacting model performance.\n",
    "\n",
    "5. **Not Suitable for All Data Types:**\n",
    "   - Some filter methods are designed for specific data types (e.g., continuous, categorical), and their effectiveness may vary across different types of datasets.\n",
    "\n",
    "6. **Limited Feature Interaction Understanding:**\n",
    "   - Filter methods do not inherently capture feature interactions, making them less suitable for problems where interactions between features are crucial, such as in non-linear relationships.\n",
    "\n",
    "7. **Insensitivity to Model Changes:**\n",
    "   - Changes in the machine learning model may not be reflected in the filter method's feature selection. If the model is changed, the filter-selected features may not remain optimal.\n",
    "\n",
    "8. **Limited Feature Engineering:**\n",
    "   - The filter method does not provide insights into feature engineering or transformation. It may not identify new features that could enhance model performance through creative combinations or transformations.\n",
    "\n",
    "9. **Not Adaptive to Model Training:**\n",
    "   - Filter methods are applied before model training and do not adapt to changes in the model during the training process. This lack of adaptability may lead to suboptimal feature selections.\n",
    "\n",
    "10. **Risk of Overlooking Important Features:**\n",
    "    - In some cases, important features that do not show strong univariate relationships with the target variable might be overlooked by the filter method.\n",
    "\n",
    "**Conclusion:**\n",
    "While the filter method is computationally efficient and serves as a quick preprocessing step, its drawbacks highlight the importance of considering more advanced feature selection methods like wrapper or embedded methods, especially when dealing with complex relationships and model-specific contexts. The choice of feature selection method should align with the characteristics of the dataset and the requirements of the machine learning task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110aed59",
   "metadata": {},
   "source": [
    "### Q5. In which situations would you prefer using the Filter method over the Wrapper method for feature selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90ac681",
   "metadata": {},
   "source": [
    "**Preferred Use of Filter Method Over Wrapper Method:**\n",
    "\n",
    "1. **Large Datasets:**\n",
    "   - When dealing with large datasets, the computational efficiency of the filter method becomes advantageous. Filter methods are generally faster and less resource-intensive compared to wrapper methods, making them suitable for large-scale datasets.\n",
    "\n",
    "2. **Quick Preprocessing:**\n",
    "   - For quick data preprocessing and exploration, especially in the initial stages of a project, filter methods can provide a rapid assessment of feature relevance without the need for extensive model training.\n",
    "\n",
    "3. **Independence of Model Choice:**\n",
    "   - If the primary goal is to identify globally relevant features independent of a specific machine learning model, the filter method may be preferred. This is particularly true when the dataset does not exhibit complex relationships that require model-specific evaluation.\n",
    "\n",
    "4. **Univariate Feature Relationships:**\n",
    "   - In situations where univariate relationships between individual features and the target variable are strong indicators of importance, the filter method can be effective. For example, when there are clear linear correlations or distinct feature distributions.\n",
    "\n",
    "5. **Exploratory Data Analysis (EDA):**\n",
    "   - During exploratory data analysis, filter methods can serve as an initial step to identify potentially significant features, allowing data scientists to focus their efforts on more detailed analyses or model-specific feature selection later in the process.\n",
    "\n",
    "6. **Less Risk of Overfitting:**\n",
    "   - In scenarios where avoiding overfitting to a specific model is a primary concern, the filter method may be chosen. Since filter methods are model-independent, there is less risk of overfitting to the peculiarities of a particular algorithm.\n",
    "\n",
    "7. **Stable Feature Importance:**\n",
    "   - If feature importance is relatively stable across different modeling approaches, the filter method can provide consistent results. This stability is particularly useful when working with diverse machine learning models.\n",
    "\n",
    "8. **Ease of Interpretability:**\n",
    "   - Filter-selected features are often easier to interpret, as their relevance is assessed independently of a specific model. This interpretability is valuable when clear insights into feature importance are required.\n",
    "\n",
    "**Conclusion:**\n",
    "The preference for using the filter method over the wrapper method depends on the specific characteristics of the dataset, the goals of the analysis, and the available computational resources. In situations where a quick, model-independent assessment of feature relevance is sufficient, the filter method can be a practical choice. However, for tasks that demand model-specific feature evaluation and are not hindered by computational constraints, wrapper or embedded methods may provide more tailored and accurate results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fb66b8",
   "metadata": {},
   "source": [
    "### Q6. In a telecom company, you are working on a project to develop a predictive model for customer churn. You are unsure of which features to include in the model because the dataset contains several different ones. Describe how you would choose the most pertinent attributes for the model using the Filter Method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0efa085",
   "metadata": {},
   "source": [
    "**Approach for Attribute Selection Using the Filter Method:**\n",
    "\n",
    "1. **Understanding the Dataset:**\n",
    "   - Begin by thoroughly understanding the dataset and the variables it contains. Explore the nature of the features, their types (categorical or numerical), and their potential relevance to customer churn.\n",
    "\n",
    "2. **Define the Target Variable:**\n",
    "   - Clearly define the target variable, in this case, whether a customer has churned or not. This variable will be used as the basis for evaluating the relevance of other features.\n",
    "\n",
    "3. **Explore Feature Correlations:**\n",
    "   - Conduct a preliminary analysis of feature correlations with the target variable. For numerical features, use correlation coefficients such as Pearson's correlation, and for categorical features, consider techniques like chi-square tests or point-biserial correlation.\n",
    "\n",
    "4. **Univariate Analysis:**\n",
    "   - Perform univariate analysis for each feature, examining how well individual features discriminate between churned and non-churned customers. Common statistical tests include t-tests for numerical features and chi-square tests for categorical features.\n",
    "\n",
    "5. **Feature Importance Metrics:**\n",
    "   - Utilize filter method metrics that provide feature importance scores. Common metrics include Information Gain, Gain Ratio, Chi-Square, F-statistic, or mutual information, depending on the nature of the features.\n",
    "\n",
    "6. **Filter by Thresholds:**\n",
    "   - Set appropriate thresholds based on the chosen metrics to filter out less relevant features. Features exceeding the threshold are considered pertinent for the predictive model.\n",
    "\n",
    "7. **Handle Redundancy:**\n",
    "   - Address any redundancy or multicollinearity issues among the selected features. If two or more features are highly correlated, consider keeping the one with higher importance or exploring dimensionality reduction techniques.\n",
    "\n",
    "8. **Iterative Refinement:**\n",
    "   - Iteratively refine the feature selection process based on feedback from model performance. After building an initial model, assess the impact of selected features on model accuracy and adjust the feature set accordingly.\n",
    "\n",
    "9. **Cross-Validation:**\n",
    "   - Validate the selected features using cross-validation techniques to ensure that the model's performance is consistent across different subsets of the data.\n",
    "\n",
    "10. **Documentation and Reporting:**\n",
    "    - Document the selected features and the rationale behind their inclusion. Provide clear reporting on the attributes chosen, their relevance, and any observed patterns.\n",
    "\n",
    "**Example Scenario:**\n",
    "   - Suppose you find that the duration of calls, monthly charges, and customer satisfaction scores have high information gain and F-statistic values, indicating their importance in predicting customer churn. You decide to include these features in your predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74262728",
   "metadata": {},
   "source": [
    "### Q7. You are working on a project to predict the outcome of a soccer match. You have a large dataset with many features, including player statistics and team rankings. Explain how you would use the Embedded method to select the most relevant features for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f752b02",
   "metadata": {},
   "source": [
    "**Utilizing the Embedded Method for Feature Selection:**\n",
    "\n",
    "1. **Choose a Machine Learning Algorithm:**\n",
    "   - Select a machine learning algorithm that inherently performs feature selection during its training process. Common algorithms with embedded feature selection capabilities include decision trees, random forests, gradient boosting machines, and certain linear models like Lasso regression.\n",
    "\n",
    "2. **Define the Target Variable:**\n",
    "   - Clearly define the target variable for your soccer match outcome prediction. This could be a binary variable indicating win/loss or a multi-class variable representing different match outcomes.\n",
    "\n",
    "3. **Prepare the Dataset:**\n",
    "   - Ensure that your dataset is properly prepared, including handling missing values, encoding categorical variables, and scaling numerical features if necessary.\n",
    "\n",
    "4. **Select an Embedded Method Algorithm:**\n",
    "   - Choose an algorithm that has embedded feature selection capabilities. For instance, if using a decision tree-based model like a random forest or gradient boosting machine, these models naturally assign importance scores to features during training.\n",
    "\n",
    "5. **Train the Model:**\n",
    "   - Train the selected machine learning algorithm on your dataset. During the training process, the algorithm will assign importance scores to each feature based on its contribution to predicting the target variable.\n",
    "\n",
    "6. **Feature Importance Scores:**\n",
    "   - Retrieve the feature importance scores generated by the model after training. These scores indicate the relative importance of each feature in influencing the model's predictions.\n",
    "\n",
    "7. **Set a Threshold:**\n",
    "   - Set a threshold for feature importance scores. Features with importance scores above the threshold are considered relevant and selected for inclusion in the final model.\n",
    "\n",
    "8. **Evaluate Model Performance:**\n",
    "   - Evaluate the performance of your model using the selected features. Utilize appropriate evaluation metrics such as accuracy, precision, recall, or F1 score to assess the model's predictive capabilities.\n",
    "\n",
    "9. **Iterative Refinement:**\n",
    "   - If necessary, iteratively refine the model by adjusting the threshold or considering interactions between features. Assess how changes in the feature set impact the model's performance.\n",
    "\n",
    "10. **Cross-Validation:**\n",
    "    - Validate the model using cross-validation techniques to ensure that the selected features contribute consistently to the model's predictive performance across different subsets of the data.\n",
    "\n",
    "11. **Documentation and Reporting:**\n",
    "    - Document the selected features and their importance scores. Provide clear reporting on the features chosen, their relevance to the soccer match outcome, and any insights gained from the feature selection process.\n",
    "\n",
    "**Example Scenario:**\n",
    "   - If using a random forest algorithm, the model might assign high importance scores to features such as team rankings, player goal statistics, and historical match performance, indicating their significance in predicting soccer match outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ee9bf8",
   "metadata": {},
   "source": [
    "### Q8. You are working on a project to predict the price of a house based on its features, such as size, location, and age. You have a limited number of features, and you want to ensure that you select the most important ones for the model. Explain how you would use the Wrapper method to select the best set of features for the predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d33eaeb",
   "metadata": {},
   "source": [
    "**Applying the Wrapper Method for Feature Selection:**\n",
    "\n",
    "1. **Define the Target Variable:**\n",
    "   - Clearly define the target variable for your house price prediction model. This will be the variable you aim to predict based on the selected features.\n",
    "\n",
    "2. **Select a Subset of Features:**\n",
    "   - Choose a subset of features to start the feature selection process. Since you have a limited number of features, include all available ones initially.\n",
    "\n",
    "3. **Choose a Performance Metric:**\n",
    "   - Select an appropriate performance metric to evaluate the model's performance during the feature selection process. Common metrics include mean squared error (MSE) for regression tasks or accuracy for classification tasks.\n",
    "\n",
    "4. **Select a Search Algorithm:**\n",
    "   - Choose a search algorithm to explore different subsets of features. Common search algorithms include exhaustive search, forward selection, backward elimination, or recursive feature elimination (RFE). The choice depends on the size of the feature space and computational resources.\n",
    "\n",
    "5. **Train and Evaluate Models:**\n",
    "   - Train a predictive model using the selected subset of features and evaluate its performance using the chosen performance metric. This involves using a machine learning algorithm, such as linear regression or a regression tree for house price prediction.\n",
    "\n",
    "6. **Iterative Feature Selection:**\n",
    "   - Iteratively add or remove features based on the performance of the model. For forward selection, start with an empty set of features and gradually add the most beneficial ones. For backward elimination, start with all features and remove the least beneficial ones.\n",
    "\n",
    "7. **Cross-Validation:**\n",
    "   - Implement cross-validation techniques to ensure that the selected features contribute consistently to the model's predictive performance across different subsets of the data. This helps avoid overfitting to a specific subset.\n",
    "\n",
    "8. **Stop Criterion:**\n",
    "   - Define a stopping criterion for the feature selection process. This could be based on achieving a certain level of model performance or when further addition or removal of features does not significantly improve the model.\n",
    "\n",
    "9. **Final Model Training:**\n",
    "   - Once the optimal set of features is identified, train the final predictive model using this subset of features. This model is expected to generalize well to new, unseen data.\n",
    "\n",
    "10. **Documentation and Reporting:**\n",
    "    - Document the selected features and their impact on the model's performance. Provide clear reporting on the features chosen, their importance in predicting house prices, and any insights gained from the wrapper method.\n",
    "\n",
    "**Example Scenario:**\n",
    "   - Through the wrapper method, you might discover that features such as location, size, and age are crucial for predicting house prices, while other features contribute less significantly.\n",
    "\n",
    "**Note:**\n",
    "   - Wrapper methods are computationally more intensive than filter methods but can provide more accurate feature subsets tailored to the specific predictive model. The choice of search algorithm and performance metric should align with the characteristics of the dataset and the goals of the prediction task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a949a9f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
