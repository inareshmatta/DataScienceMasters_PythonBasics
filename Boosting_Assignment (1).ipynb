{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca8925bd",
   "metadata": {},
   "source": [
    "## Boosting_Assignment1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98a8ede",
   "metadata": {},
   "source": [
    "Q1. What is boosting in machine learning?   \n",
    "Q2. What are the advantages and limitations of using boosting techniques?   \n",
    "Q3. Explain how boosting works.   \n",
    "Q4. What are the different types of boosting algorithms?   \n",
    "Q5. What are some common parameters in boosting algorithms?   \n",
    "Q6. How do boosting algorithms combine weak learners to create a strong learner?   \n",
    "Q7. Explain the concept of AdaBoost algorithm and its working.   \n",
    "Q8. What is the loss function used in AdaBoost algorithm?     \n",
    "Q9. How does the AdaBoost algorithm update the weights of misclassified samples?   \n",
    "Q10. What is the effect of increasing the number of estimators in AdaBoost algorithm?   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7075895",
   "metadata": {},
   "source": [
    "### Q1. What is boosting in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb24425c",
   "metadata": {},
   "source": [
    "Boosting is a machine learning ensemble technique that combines multiple weak learners to create a strong learner. It works by sequentially training a series of weak learners, with each subsequent learner focusing more on the instances that were misclassified by the previous learners. This sequential learning process allows boosting to correct errors made by earlier models, ultimately leading to improved predictive performance. Popular boosting algorithms include AdaBoost, Gradient Boosting, and XGBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2f9965",
   "metadata": {},
   "source": [
    "### Q2. What are the advantages and limitations of using boosting techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e104c5eb",
   "metadata": {},
   "source": [
    "#### Advantages and limitations of using boosting techniques:\n",
    "\n",
    "Advantages:\n",
    "1. Improved Accuracy: Boosting algorithms often yield higher accuracy compared to individual weak learners by leveraging the strengths of multiple models.\n",
    "2. Handling Complex Relationships: Boosting can effectively capture complex relationships between features and target variables, making it suitable for various types of datasets.\n",
    "3. Robustness to Overfitting: Boosting algorithms, such as Gradient Boosting and XGBoost, incorporate regularization techniques to prevent overfitting, ensuring better generalization to unseen data.\n",
    "4. Feature Importance: Boosting algorithms provide feature importance scores, allowing users to identify the most influential features in the predictive model.\n",
    "5. Versatility: Boosting algorithms can be applied to a wide range of machine learning tasks, including classification, regression, and ranking.\n",
    "\n",
    "Limitations:\n",
    "1. Sensitivity to Noisy Data: Boosting algorithms can be sensitive to noisy data and outliers, leading to degraded performance if the dataset contains significant noise.\n",
    "2. Computationally Intensive: Boosting algorithms often require more computational resources and training time compared to simpler models due to the sequential nature of training multiple weak learners.\n",
    "3. Parameter Tuning: Boosting algorithms involve tuning various hyperparameters, such as learning rate, tree depth, and the number of estimators, which can be challenging and time-consuming.\n",
    "4. Vulnerability to Overfitting: While boosting algorithms aim to reduce overfitting, they can still overfit the training data if not properly tuned or if the dataset is too small.\n",
    "5. Interpretability: Boosting models are generally less interpretable compared to simpler models like decision trees, making it challenging to explain the reasoning behind predictions.\n",
    "\n",
    "Overall, while boosting techniques offer significant advantages in terms of predictive performance and robustness, they also come with certain limitations that need to be considered when applying them to real-world problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6b9b9e",
   "metadata": {},
   "source": [
    "### Q3. Explain how boosting works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c057f7c0",
   "metadata": {},
   "source": [
    "Boosting is an ensemble learning technique that combines multiple weak learners sequentially to create a strong predictive model. The primary idea behind boosting is to sequentially train weak learners, where each subsequent model corrects the errors made by the previous ones. Here's how boosting works step by step:\n",
    "\n",
    "1. **Initialize Weights**: In the beginning, each data point in the training set is assigned an equal weight.\n",
    "\n",
    "2. **Train Weak Learner**: A weak learner, often a simple decision tree with limited depth (a stump), is trained on the dataset. The weak learner's goal is to minimize the errors in predictions.\n",
    "\n",
    "3. **Compute Error**: After training the weak learner, the algorithm computes the errors made by the model. Data points that were misclassified receive higher weights, while correctly classified points receive lower weights.\n",
    "\n",
    "4. **Adjust Weights**: The weights of the misclassified data points are increased, making them more influential in the next iteration. This emphasizes the importance of correcting the errors made by the previous model.\n",
    "\n",
    "5. **Train Next Weak Learner**: Another weak learner is trained on the same dataset, giving more weight to the previously misclassified data points. The new weak learner aims to correct the errors made by the previous model.\n",
    "\n",
    "6. **Repeat**: Steps 3 to 5 are repeated iteratively for a predefined number of iterations or until the model achieves a desired level of performance.\n",
    "\n",
    "7. **Combine Weak Learners**: Finally, all weak learners are combined to form a strong predictive model. The final prediction is typically obtained by taking a weighted average of the predictions from each weak learner, where the weights are determined based on the performance of each model.\n",
    "\n",
    "By iteratively training weak learners and focusing on correcting the errors made by the previous models, boosting effectively improves the overall predictive performance of the model. Popular boosting algorithms include AdaBoost, Gradient Boosting, XGBoost, and LightGBM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd15f4a2",
   "metadata": {},
   "source": [
    "### Q4. What are the different types of boosting algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca47dd4",
   "metadata": {},
   "source": [
    "There are several types of boosting algorithms, each with its own characteristics and approaches to improving model performance. Some of the most common types of boosting algorithms include:\n",
    "\n",
    "1. **AdaBoost (Adaptive Boosting)**: AdaBoost is one of the earliest and most popular boosting algorithms. It works by iteratively training a series of weak learners, where each subsequent learner focuses more on the instances that were misclassified by the previous ones. AdaBoost assigns higher weights to misclassified instances and lower weights to correctly classified instances, allowing subsequent weak learners to prioritize the difficult-to-classify examples.\n",
    "\n",
    "2. **Gradient Boosting**: Gradient Boosting builds a sequence of decision trees, with each tree attempting to correct the errors made by the previous ones. Unlike AdaBoost, Gradient Boosting focuses on minimizing a loss function (e.g., mean squared error for regression or log loss for classification) rather than adjusting instance weights. It uses gradient descent optimization to minimize the loss function, with each new tree added to the ensemble reducing the residual errors.\n",
    "\n",
    "3. **XGBoost (Extreme Gradient Boosting)**: XGBoost is an optimized implementation of Gradient Boosting, known for its efficiency and performance. It introduces additional regularization techniques to prevent overfitting and improve generalization. XGBoost uses a more advanced optimization algorithm, incorporates tree pruning, and supports parallel processing, making it one of the most widely used boosting algorithms for both classification and regression tasks.\n",
    "\n",
    "4. **LightGBM (Light Gradient Boosting Machine)**: LightGBM is another efficient and scalable implementation of Gradient Boosting, developed by Microsoft. It uses a novel gradient-based approach to handle categorical features and employs histogram-based algorithms for faster training speed and lower memory usage. LightGBM is particularly well-suited for large-scale datasets and has gained popularity in various machine learning competitions and real-world applications.\n",
    "\n",
    "5. **CatBoost (Categorical Boosting)**: CatBoost is a boosting algorithm developed by Yandex, designed to handle categorical features seamlessly without the need for extensive preprocessing. It employs an innovative algorithm for feature combinations and provides robust handling of categorical variables, making it suitable for a wide range of applications.\n",
    "\n",
    "These are just a few examples of boosting algorithms, and there are many other variations and implementations available. Each algorithm has its own strengths and weaknesses, and the choice of algorithm often depends on the specific requirements of the problem at hand, the size and nature of the dataset, and computational resources available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee9507c",
   "metadata": {},
   "source": [
    "### Q5. What are some common parameters in boosting algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a678f0cb",
   "metadata": {},
   "source": [
    "Boosting algorithms typically have a set of common parameters that influence the behavior and performance of the model. Some of the common parameters include:\n",
    "\n",
    "1. **Number of Trees (n_estimators)**: This parameter specifies the number of weak learners (trees) to be used in the ensemble. Increasing the number of trees may lead to better performance, but it also increases computational cost and the risk of overfitting.\n",
    "\n",
    "2. **Learning Rate (or Shrinkage)**: The learning rate controls the contribution of each tree to the final prediction. A lower learning rate requires more trees to achieve the same level of accuracy but may improve generalization. It helps to prevent overfitting by shrinking the contribution of each tree.\n",
    "\n",
    "3. **Tree Depth (max_depth)**: This parameter specifies the maximum depth of each tree in the ensemble. Deeper trees can capture more complex patterns in the data but are more prone to overfitting. Limiting the tree depth helps control model complexity and improves generalization.\n",
    "\n",
    "4. **Minimum Samples Split (min_samples_split)**: This parameter specifies the minimum number of samples required to split an internal node. It helps prevent the algorithm from creating nodes with too few samples, which can lead to overfitting.\n",
    "\n",
    "5. **Minimum Samples Leaf (min_samples_leaf)**: This parameter specifies the minimum number of samples required to be at a leaf node. It helps control the size of the trees and prevents overfitting by requiring a certain number of samples in each leaf.\n",
    "\n",
    "6. **Subsample (or Bagging Fraction)**: Subsample controls the fraction of samples used for training each tree. Setting a value less than 1.0 enables stochastic gradient boosting, where each tree is trained on a random subset of the training data. This can help improve generalization and reduce overfitting.\n",
    "\n",
    "7. **Feature Subsampling (colsample_bytree)**: This parameter controls the fraction of features (columns) used for training each tree. Similar to subsampling, it helps introduce randomness into the model and can prevent overfitting by limiting the number of features considered at each split.\n",
    "\n",
    "8. **Regularization Parameters (reg_alpha, reg_lambda)**: These parameters control L1 and L2 regularization, respectively, to prevent overfitting by penalizing large coefficients. They help to reduce model complexity and improve generalization.\n",
    "\n",
    "These are just a few examples of common parameters in boosting algorithms. The optimal values for these parameters depend on the specific dataset and problem at hand and are often determined through experimentation and cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d2c6d2",
   "metadata": {},
   "source": [
    "### Q6. How do boosting algorithms combine weak learners to create a strong learner?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c16949",
   "metadata": {},
   "source": [
    "Boosting algorithms combine multiple weak learners to create a strong learner through an iterative process. Here's how it typically works:\n",
    "\n",
    "1. **Sequential Training**: Boosting algorithms train a sequence of weak learners (usually decision trees) sequentially. Each tree is trained using the information from the previous trees in the ensemble.\n",
    "\n",
    "2. **Focus on Errors**: During each iteration, the boosting algorithm assigns higher weights to the instances that were misclassified by the previous weak learners. This allows subsequent weak learners to focus more on the difficult-to-classify instances.\n",
    "\n",
    "3. **Weighted Voting**: After each weak learner is trained, it contributes to the final prediction by assigning it a weight based on its performance. Weak learners that perform well on the training data are given higher weights, indicating their importance in the ensemble.\n",
    "\n",
    "4. **Aggregate Predictions**: The predictions from all weak learners are combined using a weighted sum or a voting mechanism to produce the final prediction of the ensemble. Typically, each weak learner's prediction is weighted by its performance during training.\n",
    "\n",
    "5. **Adaptive Learning**: Boosting algorithms adaptively adjust the weights of training instances at each iteration, focusing more on the instances that are difficult to classify correctly. This iterative process gradually improves the overall performance of the ensemble.\n",
    "\n",
    "By iteratively combining weak learners and focusing on the errors made by previous learners, boosting algorithms can create a strong learner that achieves high accuracy on the training data and generalizes well to unseen data. The final model tends to have lower bias and variance compared to individual weak learners, making it robust and effective for a wide range of classification and regression tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61a534e",
   "metadata": {},
   "source": [
    "### Q7. Explain the concept of AdaBoost algorithm and its working."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc36873",
   "metadata": {},
   "source": [
    "AdaBoost, short for Adaptive Boosting, is a popular boosting algorithm used for classification tasks. It works by combining multiple weak learners (typically decision trees) to create a strong learner. Here's how AdaBoost works:\n",
    "\n",
    "1. **Initialization**: At the beginning, each training instance is assigned an equal weight. These weights are used to give more importance to instances that were misclassified by the previous weak learners.\n",
    "\n",
    "2. **Sequential Training**: AdaBoost trains a sequence of weak learners (decision trees) sequentially. In each iteration, it focuses more on the instances that were misclassified by the previous weak learners.\n",
    "\n",
    "3. **Weighted Voting**: After each weak learner is trained, it predicts the class labels for all instances in the training data. The algorithm then calculates the weighted error rate, which is the sum of the weights of the misclassified instances.\n",
    "\n",
    "4. **Classifier Weight**: Based on the weighted error rate, AdaBoost assigns a weight to each weak learner. Weak learners with lower error rates are given higher weights, indicating their importance in the final ensemble.\n",
    "\n",
    "5. **Update Instance Weights**: AdaBoost updates the weights of the training instances. It increases the weights of misclassified instances, forcing the next weak learner to focus more on them.\n",
    "\n",
    "6. **Final Prediction**: Once all weak learners are trained, AdaBoost combines their predictions using a weighted sum or a voting mechanism. The weights of the weak learners determine their influence on the final prediction. Typically, stronger weak learners have higher weights.\n",
    "\n",
    "7. **AdaBoost Algorithm**: The AdaBoost algorithm iteratively repeats steps 2 to 6 until a predefined number of weak learners is reached or until a desired level of accuracy is achieved. Each weak learner is trained on a modified version of the training data, where the weights of the misclassified instances are adjusted.\n",
    "\n",
    "By combining multiple weak learners in a sequential manner and focusing on the instances that are difficult to classify, AdaBoost creates a strong ensemble classifier that often outperforms individual weak learners. The final model tends to have high accuracy and robustness against overfitting, making AdaBoost a powerful algorithm for classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e86617",
   "metadata": {},
   "source": [
    "### Q8. What is the loss function used in AdaBoost algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5053ab5",
   "metadata": {},
   "source": [
    "In AdaBoost (Adaptive Boosting) algorithm, the loss function used is the exponential loss function, also known as the exponential loss or exponential loss criterion. \n",
    "\n",
    "The exponential loss function is defined as:\n",
    "\n",
    "$ L(y, f(x)) = e^{-y \\cdot f(x)} $\n",
    "\n",
    "Where:\n",
    "- $( y )$ is the true label of the instance $( y = +1 )$ for positive class, $( y = -1 )$ for negative class).\n",
    "- $( f(x) )$ is the prediction made by the weak learner for the instance $( x )$.\n",
    "\n",
    "The exponential loss function assigns higher penalties to misclassifications, especially for instances that are misclassified with high confidence. It amplifies the impact of misclassifications, causing the boosting algorithm to focus more on correcting these errors in subsequent iterations.\n",
    "\n",
    "The exponential loss function is particularly effective in AdaBoost because it allows the algorithm to assign higher weights to misclassified instances in each iteration, ensuring that subsequent weak learners focus more on correcting these mistakes. As a result, AdaBoost can progressively improve the model's performance and achieve higher accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdfca4b",
   "metadata": {},
   "source": [
    "### Q9. How does the AdaBoost algorithm update the weights of misclassified samples?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6e7965",
   "metadata": {},
   "source": [
    "In the AdaBoost algorithm, the weights of misclassified samples are updated in each iteration to prioritize the correct classification of these instances in subsequent iterations. The process of updating weights involves the following steps:\n",
    "\n",
    "1. **Initialize Weights**: Initially, all training samples are assigned equal weights. These weights determine the importance of each sample in the training process.\n",
    "\n",
    "2. **Train Weak Learner**: In each iteration, a weak learner (e.g., decision stump) is trained on the training data. The weak learner aims to minimize the weighted error rate, where the weights are initially assigned to each sample.\n",
    "\n",
    "3. **Calculate Weighted Error Rate**: After training the weak learner, its performance is evaluated on the training data. The weighted error rate is calculated based on the misclassification of each sample. Samples that are misclassified are assigned higher weights.\n",
    "\n",
    "4. **Update Sample Weights**: The weights of the misclassified samples are updated to increase their importance in the subsequent iteration. The update formula for the weight of each sample depends on its misclassification status:\n",
    "   - For correctly classified samples: Their weights are decreased to de-emphasize them in the next iteration.\n",
    "   - For misclassified samples: Their weights are increased to prioritize them in the next iteration.\n",
    "\n",
    "5. **Normalize Weights**: After updating the weights, they are normalized to ensure that they sum up to 1. This normalization step maintains the relative importance of the samples while preventing the weights from growing too large or too small.\n",
    "\n",
    "6. **Repeat Iterations**: Steps 2-5 are repeated for a specified number of iterations or until a predefined stopping criterion is met (e.g., achieving a desired level of accuracy).\n",
    "\n",
    "By updating the weights of misclassified samples in each iteration, AdaBoost focuses on difficult-to-classify instances, making subsequent weak learners pay more attention to these samples. This adaptive boosting of misclassified samples helps improve the overall performance of the ensemble model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ee2c66",
   "metadata": {},
   "source": [
    "### Q10. What is the effect of increasing the number of estimators in AdaBoost algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37189d1",
   "metadata": {},
   "source": [
    "Increasing the number of estimators (weak learners) in the AdaBoost algorithm typically has the following effects:\n",
    "\n",
    "1. **Improved Performance**: Generally, increasing the number of estimators leads to improved performance of the AdaBoost ensemble. With more weak learners, the ensemble can capture more complex patterns in the data, resulting in better generalization and lower bias.\n",
    "\n",
    "2. **Reduced Bias**: As the number of estimators increases, the AdaBoost ensemble becomes more flexible and capable of fitting the training data more closely. This reduction in bias allows the model to capture intricate relationships between features and the target variable, leading to a better approximation of the true underlying function.\n",
    "\n",
    "3. **Potential Overfitting**: While increasing the number of estimators can enhance model performance on the training data, there is a risk of overfitting, especially if the dataset is small or noisy. As the model becomes more complex, it may start to memorize noise or outliers in the training data, resulting in decreased generalization performance on unseen data.\n",
    "\n",
    "4. **Slower Training**: Training a larger number of weak learners can significantly increase the computational time required to fit the AdaBoost ensemble. Each additional estimator adds computational overhead, as the algorithm iteratively updates sample weights and trains new weak learners in each iteration.\n",
    "\n",
    "5. **Diminishing Returns**: There may be diminishing returns in terms of performance improvement with each additional estimator beyond a certain point. After reaching a sufficient number of estimators, further increasing their count may lead to marginal improvements or even degradation in performance due to overfitting or increased computational complexity.\n",
    "\n",
    "In summary, while increasing the number of estimators in the AdaBoost algorithm can enhance performance and reduce bias, it is essential to monitor for signs of overfitting and consider the trade-offs between model complexity, computational resources, and generalization performance. Cross-validation and model evaluation on a separate validation set can help determine the optimal number of estimators for a given dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4da191",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5aaff3",
   "metadata": {},
   "source": [
    "## Boosting_Assignmnent_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52363f0",
   "metadata": {},
   "source": [
    "Q1. What is Gradient Boosting Regression?  \n",
    "Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a\n",
    "simple regression problem as an example and train the model on a small dataset. Evaluate the model's\n",
    "performance using metrics such as mean squared error and R-squared.   \n",
    "Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to\n",
    "optimise the performance of the model. Use grid search or random search to find the best\n",
    "hyperparameters    \n",
    "Q4. What is a weak learner in Gradient Boosting?   \n",
    "Q5. What is the intuition behind the Gradient Boosting algorithm?   \n",
    "Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?  \n",
    "Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting\n",
    "algorithm?  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a20c0b4",
   "metadata": {},
   "source": [
    "### Q1. What is Gradient Boosting Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1062c98f",
   "metadata": {},
   "source": [
    "Gradient Boosting Regression is a machine learning technique used for regression tasks, where the goal is to predict continuous numerical values. It is an ensemble learning method that builds a strong predictive model by combining multiple weak predictive models, typically decision trees, in a sequential manner. \n",
    "\n",
    "In Gradient Boosting Regression, the model is built in an iterative fashion. Initially, a simple model is created to make predictions on the training data. In subsequent iterations, additional models, called \"weak learners,\" are trained to correct the errors made by the previous models. Each new model focuses on learning from the residuals (the differences between the actual target values and the predictions made by the previous models) in order to reduce the overall error of the ensemble.\n",
    "\n",
    "The \"gradient\" in Gradient Boosting refers to the technique of minimizing the loss function by using gradient descent optimization. In each iteration, the algorithm calculates the gradients of the loss function with respect to the predictions of the previous model, and then fits a new model to the negative gradients (i.e., the residuals) to update the predictions. This process is repeated for a specified number of iterations or until a stopping criterion is met.\n",
    "\n",
    "Overall, Gradient Boosting Regression is known for its high predictive accuracy and robustness against overfitting. It is widely used in various regression tasks, such as predicting house prices, stock prices, and customer churn rates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201422fa",
   "metadata": {},
   "source": [
    "### Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0b77e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE: 5.644063286794225e-09\n",
      "Training R2 score: 0.9999999992944921\n",
      "Test MSE: 18.66751664271918\n",
      "Test R2 score: -6.000318741019692\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DecisionTreeRegressor:\n",
    "    def __init__(self, max_depth=None):\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.n_features_ = X.shape[1]\n",
    "        self.tree_ = self._build_tree(X, y)\n",
    "\n",
    "    def _build_tree(self, X, y, depth=0):\n",
    "        n_samples, n_features = X.shape\n",
    "        if depth >= self.max_depth or n_samples <= 1:\n",
    "            return np.mean(y)\n",
    "\n",
    "        feature_idxs = np.random.choice(n_features, min(n_features, int(np.sqrt(n_features))), replace=False)\n",
    "        best_feature, best_threshold, best_gain = None, None, -np.inf\n",
    "\n",
    "        for feature_idx in feature_idxs:\n",
    "            thresholds = np.unique(X[:, feature_idx])\n",
    "            for threshold in thresholds:\n",
    "                left_indices = np.where(X[:, feature_idx] <= threshold)[0]\n",
    "                right_indices = np.where(X[:, feature_idx] > threshold)[0]\n",
    "                if len(left_indices) == 0 or len(right_indices) == 0:\n",
    "                    continue\n",
    "                gain = self._compute_gain(y, y[left_indices], y[right_indices])\n",
    "                if gain > best_gain:\n",
    "                    best_feature = feature_idx\n",
    "                    best_threshold = threshold\n",
    "                    best_gain = gain\n",
    "\n",
    "        if best_gain == -np.inf:\n",
    "            return np.mean(y)\n",
    "\n",
    "        left_indices = np.where(X[:, best_feature] <= best_threshold)[0]\n",
    "        right_indices = np.where(X[:, best_feature] > best_threshold)[0]\n",
    "        left_subtree = self._build_tree(X[left_indices], y[left_indices], depth + 1)\n",
    "        right_subtree = self._build_tree(X[right_indices], y[right_indices], depth + 1)\n",
    "\n",
    "        return {'feature': best_feature, 'threshold': best_threshold,\n",
    "                'left': left_subtree, 'right': right_subtree}\n",
    "\n",
    "    def _compute_gain(self, parent, left_child, right_child):\n",
    "        total_samples = len(parent)\n",
    "        p_left = len(left_child) / total_samples\n",
    "        p_right = len(right_child) / total_samples\n",
    "        return self._mse(parent) - (p_left * self._mse(left_child) + p_right * self._mse(right_child))\n",
    "\n",
    "    def _mse(self, y):\n",
    "        return np.mean((y - np.mean(y))**2)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._predict_tree(x, self.tree_) for x in X])\n",
    "\n",
    "    def _predict_tree(self, x, tree):\n",
    "        if isinstance(tree, dict):\n",
    "            if x[tree['feature']] <= tree['threshold']:\n",
    "                return self._predict_tree(x, tree['left'])\n",
    "            else:\n",
    "                return self._predict_tree(x, tree['right'])\n",
    "        else:\n",
    "            return tree\n",
    "\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.models = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.base_prediction = np.mean(y)\n",
    "        previous_prediction = np.full_like(y, self.base_prediction, dtype=float)\n",
    "        self.models.append(previous_prediction)\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            residuals = y - previous_prediction\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X, residuals)\n",
    "            prediction = tree.predict(X)\n",
    "            previous_prediction += self.learning_rate * prediction\n",
    "            self.models.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = np.array([self.base_prediction] * len(X))\n",
    "        for tree in self.models[1:]:\n",
    "            predictions += self.learning_rate * tree.predict(X)\n",
    "        return predictions\n",
    "\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred)**2)\n",
    "\n",
    "def r2_score(y_true, y_pred):\n",
    "    numerator = np.sum((y_true - y_pred)**2)\n",
    "    denominator = np.sum((y_true - np.mean(y_true))**2)\n",
    "    return 1 - (numerator / denominator)\n",
    "\n",
    "# Example usage:\n",
    "X_train = np.array([[1], [2], [3], [4], [5]])\n",
    "y_train = np.array([2, 4, 6, 8, 10])\n",
    "\n",
    "X_test = np.array([[6], [7], [8]])\n",
    "y_test = np.array([12, 14, 16])\n",
    "\n",
    "gb_regressor = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "gb_regressor.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train = gb_regressor.predict(X_train)\n",
    "mse_train = mean_squared_error(y_train, y_pred_train)\n",
    "r2_train = r2_score(y_train, y_pred_train)\n",
    "\n",
    "print(\"Training MSE:\", mse_train)\n",
    "print(\"Training R2 score:\", r2_train)\n",
    "\n",
    "y_pred_test = gb_regressor.predict(X_test)\n",
    "mse_test = mean_squared_error(y_test, y_pred_test)\n",
    "r2_test = r2_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"Test MSE:\", mse_test)\n",
    "print(\"Test R2 score:\", r2_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486ecc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load the California housing dataset\n",
    "california_housing = fetch_california_housing()\n",
    "data = california_housing.data\n",
    "target = california_housing.target\n",
    "feature_names = california_housing.feature_names\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define numerical and categorical features\n",
    "numeric_features = []\n",
    "categorical_features = []\n",
    "for i, dtype in enumerate(california_housing.feature_names):\n",
    "    if np.issubdtype(data[:, i].dtype, np.number):\n",
    "        numeric_features.append(i)\n",
    "    else:\n",
    "        categorical_features.append(i)\n",
    "\n",
    "# Define preprocessing steps\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define parameter grids for grid search\n",
    "param_grid_adaboost = {\n",
    "    'regressor__n_estimators': [50, 100, 200],\n",
    "    'regressor__learning_rate': [0.01, 0.1, 1.0],\n",
    "    'regressor__loss': ['linear', 'square', 'exponential']\n",
    "}\n",
    "\n",
    "param_grid_gradientboost = {\n",
    "    'regressor__n_estimators': [50, 100, 200],\n",
    "    'regressor__learning_rate': [0.01, 0.1, 1.0],\n",
    "    'regressor__max_depth': [3, 4, 5],\n",
    "    'regressor__min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "param_grid_xgboost = {\n",
    "    'regressor__n_estimators': [50, 100, 200],\n",
    "    'regressor__learning_rate': [0.01, 0.1, 1.0],\n",
    "    'regressor__max_depth': [3, 4, 5],\n",
    "    'regressor__min_child_weight': [1, 3, 5]\n",
    "}\n",
    "\n",
    "# Define pipelines for each algorithm\n",
    "ada_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', AdaBoostRegressor())\n",
    "])\n",
    "\n",
    "gb_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', GradientBoostingRegressor())\n",
    "])\n",
    "\n",
    "xgb_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', XGBRegressor())\n",
    "])\n",
    "\n",
    "# Perform grid search for AdaBoost\n",
    "ada_grid_search = GridSearchCV(ada_pipeline, param_grid_adaboost, cv=5, scoring='neg_mean_squared_error')\n",
    "ada_grid_search.fit(X_train, y_train)\n",
    "print(\"AdaBoost Best Parameters:\", ada_grid_search.best_params_)\n",
    "\n",
    "# Perform grid search for Gradient Boosting\n",
    "gb_grid_search = GridSearchCV(gb_pipeline, param_grid_gradientboost, cv=5, scoring='neg_mean_squared_error')\n",
    "gb_grid_search.fit(X_train, y_train)\n",
    "print(\"Gradient Boosting Best Parameters:\", gb_grid_search.best_params_)\n",
    "\n",
    "# Perform grid search for XGBoost\n",
    "xgb_grid_search = GridSearchCV(xgb_pipeline, param_grid_xgboost, cv=5, scoring='neg_mean_squared_error')\n",
    "xgb_grid_search.fit(X_train, y_train)\n",
    "print(\"XGBoost Best Parameters:\", xgb_grid_search.best_params_)\n",
    "\n",
    "# Evaluate models on test data\n",
    "ada_best = ada_grid_search.best_estimator_\n",
    "gb_best = gb_grid_search.best_estimator_\n",
    "xgb_best = xgb_grid_search.best_estimator_\n",
    "\n",
    "y_pred_ada = ada_best.predict(X_test)\n",
    "y_pred_gb = gb_best.predict(X_test)\n",
    "y_pred_xgb = xgb_best.predict(X_test)\n",
    "\n",
    "mse_ada = mean_squared_error(y_test, y_pred_ada)\n",
    "r2_ada = r2_score(y_test, y_pred_ada)\n",
    "mse_gb = mean_squared_error(y_test, y_pred_gb)\n",
    "r2_gb = r2_score(y_test, y_pred_gb)\n",
    "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
    "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
    "\n",
    "print(\"AdaBoost MSE:\", mse_ada)\n",
    "print(\"AdaBoost R-squared:\", r2_ada)\n",
    "print(\"Gradient Boosting MSE:\", mse_gb)\n",
    "print(\"Gradient Boosting R-squared:\", r2_gb)\n",
    "print(\"XGBoost MSE:\", mse_xgb)\n",
    "print(\"XGBoost R-squared:\", r2_xgb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc6b2d4",
   "metadata": {},
   "source": [
    "### Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimise the performance of the model. Use grid search or random search to find the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75fe3229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Best Parameters: {'regressor__learning_rate': 0.1, 'regressor__n_estimators': 50}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5660/1268444815.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;31m# Perform grid search for Gradient Boosting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[0mgb_grid_search\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgb_pipeline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_grid_gradientboost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'neg_mean_squared_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m \u001b[0mgb_grid_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Gradient Boosting Best Parameters:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgb_grid_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1150\u001b[0m                 )\n\u001b[0;32m   1151\u001b[0m             ):\n\u001b[1;32m-> 1152\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    896\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 898\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    899\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    900\u001b[0m             \u001b[1;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1420\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1421\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1422\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1423\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1424\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    843\u001b[0m                     )\n\u001b[0;32m    844\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 845\u001b[1;33m                 out = parallel(\n\u001b[0m\u001b[0;32m    846\u001b[0m                     delayed(_fit_and_score)(\n\u001b[0;32m    847\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         )\n\u001b[1;32m---> 65\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1861\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1862\u001b[0m             \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1863\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1864\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1865\u001b[0m         \u001b[1;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1790\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1791\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1792\u001b[1;33m                 \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1793\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1794\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    727\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    728\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 729\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    730\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    731\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1150\u001b[0m                 )\n\u001b[0;32m   1151\u001b[0m             ):\n\u001b[1;32m-> 1152\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    425\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"passthrough\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    426\u001b[0m                 \u001b[0mfit_params_last_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_params_steps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 427\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params_last_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    428\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1150\u001b[0m                 )\n\u001b[0;32m   1151\u001b[0m             ):\n\u001b[1;32m-> 1152\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[0;32m    530\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;31m# fit the boosting stages\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m         n_stages = self._fit_stages(\n\u001b[0m\u001b[0;32m    533\u001b[0m             \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\u001b[0m in \u001b[0;36m_fit_stages\u001b[1;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor)\u001b[0m\n\u001b[0;32m    608\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m             \u001b[1;31m# fit next stage of trees\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m             raw_predictions = self._fit_stage(\n\u001b[0m\u001b[0;32m    611\u001b[0m                 \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\u001b[0m in \u001b[0;36m_fit_stage\u001b[1;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_csc, X_csr)\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_csr\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mX_csr\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 245\u001b[1;33m             \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresidual\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m             \u001b[1;31m# update tree leaves\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1150\u001b[0m                 )\n\u001b[0;32m   1151\u001b[0m             ):\n\u001b[1;32m-> 1152\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m   1318\u001b[0m         \"\"\"\n\u001b[0;32m   1319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1320\u001b[1;33m         super()._fit(\n\u001b[0m\u001b[0;32m   1321\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1322\u001b[0m             \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    441\u001b[0m             )\n\u001b[0;32m    442\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 443\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmissing_values_in_feature_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load the California housing dataset\n",
    "california_housing = fetch_california_housing()\n",
    "data = california_housing.data\n",
    "target = california_housing.target\n",
    "feature_names = california_housing.feature_names\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define preprocessing steps\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Define parameter grids for grid search\n",
    "param_grid_adaboost = {\n",
    "    'regressor__n_estimators': [50, 100, 200],\n",
    "    'regressor__learning_rate': [0.01, 0.1, 1.0]\n",
    "}\n",
    "\n",
    "param_grid_gradientboost = {\n",
    "    'regressor__n_estimators': [50, 100, 200],\n",
    "    'regressor__learning_rate': [0.01, 0.1, 1.0],\n",
    "    'regressor__max_depth': [3, 4, 5]\n",
    "}\n",
    "\n",
    "param_grid_xgboost = {\n",
    "    'regressor__n_estimators': [50, 100, 200],\n",
    "    'regressor__learning_rate': [0.01, 0.1, 1.0],\n",
    "    'regressor__max_depth': [3, 4, 5]\n",
    "}\n",
    "\n",
    "# Define pipelines for each algorithm\n",
    "ada_pipeline = Pipeline([\n",
    "    ('scaler', scaler),\n",
    "    ('regressor', AdaBoostRegressor())\n",
    "])\n",
    "\n",
    "gb_pipeline = Pipeline([\n",
    "    ('scaler', scaler),\n",
    "    ('regressor', GradientBoostingRegressor())\n",
    "])\n",
    "\n",
    "xgb_pipeline = Pipeline([\n",
    "    ('scaler', scaler),\n",
    "    ('regressor', XGBRegressor())\n",
    "])\n",
    "\n",
    "# Perform grid search for AdaBoost\n",
    "ada_grid_search = GridSearchCV(ada_pipeline, param_grid_adaboost, cv=5, scoring='neg_mean_squared_error')\n",
    "ada_grid_search.fit(X_train, y_train)\n",
    "print(\"AdaBoost Best Parameters:\", ada_grid_search.best_params_)\n",
    "\n",
    "# Perform grid search for Gradient Boosting\n",
    "gb_grid_search = GridSearchCV(gb_pipeline, param_grid_gradientboost, cv=5, scoring='neg_mean_squared_error')\n",
    "gb_grid_search.fit(X_train, y_train)\n",
    "print(\"Gradient Boosting Best Parameters:\", gb_grid_search.best_params_)\n",
    "\n",
    "# Perform grid search for XGBoost\n",
    "xgb_grid_search = GridSearchCV(xgb_pipeline, param_grid_xgboost, cv=5, scoring='neg_mean_squared_error')\n",
    "xgb_grid_search.fit(X_train, y_train)\n",
    "print(\"XGBoost Best Parameters:\", xgb_grid_search.best_params_)\n",
    "\n",
    "# Evaluate models on test data\n",
    "ada_best = ada_grid_search.best_estimator_\n",
    "gb_best = gb_grid_search.best_estimator_\n",
    "xgb_best = xgb_grid_search.best_estimator_\n",
    "\n",
    "y_pred_ada = ada_best.predict(X_test)\n",
    "y_pred_gb = gb_best.predict(X_test)\n",
    "y_pred_xgb = xgb_best.predict(X_test)\n",
    "\n",
    "mse_ada = mean_squared_error(y_test, y_pred_ada)\n",
    "r2_ada = r2_score(y_test, y_pred_ada)\n",
    "mse_gb = mean_squared_error(y_test, y_pred_gb)\n",
    "r2_gb = r2_score(y_test, y_pred_gb)\n",
    "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
    "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
    "\n",
    "print(\"AdaBoost MSE:\", mse_ada)\n",
    "print(\"AdaBoost R-squared:\", r2_ada)\n",
    "print(\"Gradient Boosting MSE:\", mse_gb)\n",
    "print(\"Gradient Boosting R-squared:\", r2_gb)\n",
    "print(\"XGBoost MSE:\", mse_xgb)\n",
    "print(\"XGBoost R-squared:\", r2_xgb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8db36c0",
   "metadata": {},
   "source": [
    "### Q4. What is a weak learner in Gradient Boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e795f7",
   "metadata": {},
   "source": [
    "In Gradient Boosting, a weak learner refers to a base model that is simple and performs slightly better than random guessing on a given problem. Weak learners are typically models with low predictive power, such as shallow decision trees or linear models.\n",
    "\n",
    "The key characteristic of a weak learner is that it can only provide marginal improvements to the model's predictive performance. However, when combined in an ensemble with other weak learners using boosting, they can collectively create a strong predictive model.\n",
    "\n",
    "Gradient Boosting works by sequentially adding weak learners to the ensemble, with each new learner focusing on the mistakes made by the previous ones. By iteratively fitting new weak learners to the residuals (the differences between the actual and predicted values) of the previous predictions, Gradient Boosting gradually reduces the error of the overall ensemble, ultimately resulting in a powerful predictive model.\n",
    "\n",
    "Therefore, in Gradient Boosting, the weak learners serve as building blocks that, when combined intelligently, contribute to the creation of a strong predictive model capable of capturing complex relationships within the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567c04f2",
   "metadata": {},
   "source": [
    "### Q5. What is the intuition behind the Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd1e2a4",
   "metadata": {},
   "source": [
    "The intuition behind the Gradient Boosting algorithm lies in the concept of improving the performance of a weak learner by iteratively adding new weak learners that focus on the mistakes made by the previous ones.\n",
    "\n",
    "Here's a step-by-step intuition behind how Gradient Boosting works:\n",
    "\n",
    "1. **Initialization**: Initially, the model starts with an initial weak learner, which could be any simple model that performs slightly better than random guessing, such as a decision tree with very few levels.\n",
    "\n",
    "2. **Sequential Training**: Gradient Boosting adds weak learners sequentially to the ensemble. Each new weak learner is trained to correct the errors (residuals) made by the combination of all the previous weak learners.\n",
    "\n",
    "3. **Gradient Descent**: At each iteration, the algorithm calculates the gradient of the loss function with respect to the current ensemble's predictions. This gradient provides information on how to update the predictions to reduce the overall loss.\n",
    "\n",
    "4. **Model Fitting**: The new weak learner is trained to predict the negative gradient (or pseudo-residuals) of the loss function with respect to the current predictions. Essentially, the new learner focuses on capturing the remaining patterns or errors in the data that the ensemble has not yet captured.\n",
    "\n",
    "5. **Combining Predictions**: After training the new weak learner, its predictions are added to the ensemble by weighting them according to a learning rate parameter. This learning rate controls the contribution of each weak learner to the final prediction.\n",
    "\n",
    "6. **Iterative Process**: Steps 3 to 5 are repeated iteratively for a predefined number of iterations or until a stopping criterion is met. With each iteration, the ensemble becomes increasingly adept at capturing the complex relationships in the data and reducing the overall error.\n",
    "\n",
    "7. **Final Prediction**: The final prediction is obtained by aggregating the predictions of all weak learners in the ensemble, typically through a weighted sum.\n",
    "\n",
    "By iteratively adding new weak learners that focus on the residuals of the previous predictions, Gradient Boosting gradually reduces the error of the ensemble, resulting in a strong predictive model capable of capturing complex relationships in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c56fa3",
   "metadata": {},
   "source": [
    "### Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af42720",
   "metadata": {},
   "source": [
    "The Gradient Boosting algorithm builds an ensemble of weak learners sequentially through an iterative process. Here's a step-by-step explanation of how it constructs the ensemble:\n",
    "\n",
    "1. **Initialization**: Gradient Boosting starts with an initial weak learner, which is typically a simple model that performs slightly better than random guessing, such as a decision tree with very few levels.\n",
    "\n",
    "2. **Initial Prediction**: The initial weak learner makes predictions on the training data.\n",
    "\n",
    "3. **Residual Calculation**: The algorithm calculates the residuals, which represent the difference between the actual target values and the predictions made by the current ensemble.\n",
    "\n",
    "4. **Learning from Residuals**: A new weak learner is trained to predict the residuals (or negative gradients) of the loss function with respect to the current predictions. This weak learner focuses on capturing the remaining patterns or errors in the data that the ensemble has not yet captured.\n",
    "\n",
    "5. **Updating Predictions**: The predictions of the new weak learner are added to the predictions of the current ensemble, typically by combining them with a learning rate parameter. This learning rate controls the contribution of each weak learner to the final prediction.\n",
    "\n",
    "6. **Iterative Process**: Steps 3 to 5 are repeated iteratively for a predefined number of iterations or until a stopping criterion is met. With each iteration, a new weak learner is trained to predict the residuals of the current ensemble, and its predictions are added to the ensemble.\n",
    "\n",
    "7. **Final Prediction**: The final prediction is obtained by aggregating the predictions of all weak learners in the ensemble, typically through a weighted sum.\n",
    "\n",
    "By iteratively adding new weak learners that focus on the residuals of the previous predictions, Gradient Boosting gradually reduces the error of the ensemble, resulting in a strong predictive model capable of capturing complex relationships in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e131a530",
   "metadata": {},
   "source": [
    "### Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d820a7c",
   "metadata": {},
   "source": [
    "Constructing the mathematical intuition behind the Gradient Boosting algorithm involves several key steps:\n",
    "\n",
    "1. **Understanding Gradient Descent**: Gradient Boosting is based on the concept of gradient descent, a widely used optimization technique. Begin by understanding how gradient descent works, particularly in the context of minimizing a loss function.\n",
    "\n",
    "2. **Loss Functions**: Learn about different loss functions commonly used in regression and classification problems, such as mean squared error (MSE) for regression and log loss (cross-entropy) for classification.\n",
    "\n",
    "3. **Gradient Descent for Regression**: Understand how gradient descent can be applied to minimize the loss function in regression problems. This involves computing the gradient (derivative) of the loss function with respect to the model predictions and updating the model parameters (coefficients) in the direction that minimizes the loss.\n",
    "\n",
    "4. **Gradient Boosting for Regression**: Build intuition for how Gradient Boosting extends gradient descent for regression problems by sequentially adding weak learners (decision trees) to the ensemble. Each weak learner is trained to predict the residuals (negative gradients) of the loss function with respect to the current ensemble predictions.\n",
    "\n",
    "5. **Boosting Algorithm**: Study the boosting algorithm step-by-step, focusing on how each weak learner is trained and added to the ensemble to reduce the error of the overall model.\n",
    "\n",
    "6. **Learning Rate**: Understand the role of the learning rate parameter in controlling the contribution of each weak learner to the final prediction. Lower learning rates result in more conservative updates and smoother convergence.\n",
    "\n",
    "7. **Regularization**: Learn about regularization techniques used in Gradient Boosting, such as shrinkage (learning rate) and tree-specific parameters (tree depth, number of leaves, etc.), to prevent overfitting and improve generalization performance.\n",
    "\n",
    "8. **Gradient Boosting for Classification**: Extend the intuition to classification problems by considering how Gradient Boosting can be adapted to minimize classification loss functions (e.g., log loss) and make predictions for multiple classes.\n",
    "\n",
    "By following these steps and gaining a solid understanding of the underlying principles, you can develop a comprehensive mathematical intuition for the Gradient Boosting algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cd3081",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "599c841a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "799ce2ce",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "96e853f2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "81902705",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "144e3284",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "506d87e7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c19190fe",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d8f6a03",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
